[
    {
        "original_question": "From the book's perspective, what are the fundamental structural and performance differences between a standard binary search tree and a B-tree? Why are B-trees preferred for disk-based storage systems?",
        "decomposed_queries_text": "1.  **What are the fundamental structural differences between a standard binary search tree and a B-tree, according to the book?**\n2.  **What are the fundamental performance differences between a standard binary search tree and a B-tree, according to the book?**\n3.  **Why are B-trees preferred for disk-based storage systems, according to the book?**",
        "hypothetical_document": "二元搜尋樹 (BST) 結構簡單，但其搜尋、插入和刪除操作在最壞情況下可達到 O(n) 的時間複雜度。 B-樹，尤其是 B+ 樹，透過平衡的樹結構和高扇出度，保證了 O(log n) 的操作複雜度，降低了磁碟 I/O 次數，使其成為基於磁碟儲存系統的理想選擇。 B+ 樹將所有資料儲存在葉節點，並在內部節點中僅儲存索引鍵，進一步最佳化了資料存取效率。",
        "query_for_retrieval": "二元搜尋樹 (BST) 結構簡單，但其搜尋、插入和刪除操作在最壞情況下可達到 O(n) 的時間複雜度。 B-樹，尤其是 B+ 樹，透過平衡的樹結構和高扇出度，保證了 O(log n) 的操作複雜度，降低了磁碟 I/O 次數，使其成為基於磁碟儲存系統的理想選擇。 B+ 樹將所有資料儲存在葉節點，並在內部節點中僅儲存索引鍵，進一步最佳化了資料存取效率。",
        "retrieved_contexts_from_server": [
            {
                "text": "exact height of a B-tree can be considerably less than that of a red-black tree,\nhowever, because its branching factor, and hence the base of the logarithm that\nexpresses its height, can be much larger. Therefore, we can also use B-trees to\nimplement many dynamic-set operations in time O.lg n/.\nB-trees generalize binary search trees in a natural manner. Figure 18.1 shows a\nsimple B-tree. If an internal B-tree node x contains x:n keys, then x has x:n C 1\nchildren. The keys in node x serve as dividing points separating the range of keys\nhandled by x into x:n C 1 subranges, each handled by one child of x. When\nsearching for a key in a B-tree, we make an .x:n C 1/-way decision based on\ncomparisons with the x:n keys stored at node x. The structure of leaf nodes differs\nfrom that of internal nodes; we will examine these differences in Section 18.1.\nSection 18.1 gives a precise deﬁnition of B-trees and proves that the height of\na B-tree grows only logarithmically with the number of nodes it contains. Sec-\ntion 18.2 describes how to search for a key and insert a key into a B-tree, and\nSection 18.3 discusses deletion. Before proceeding, however, we need to ask why\nwe evaluate data structures designed to work on a disk differently from data struc-\ntures designed to work in main random-access memory.\nData structures on secondary storage\nComputer systems take advantage of various technologies that provide memory\ncapacity. The primary memory (or main memory) of a computer system normally\nChapter 18\nB-Trees\n485\nB\nC\nF\nG\nJ\nK\nL\nD H\nN\nP\nR\nS\nV W\nY\nZ\nQ\nT\nX\nM\nT:root\nFigure 18.1\nA B-tree whose keys are the consonants of English. An internal node x containing\nx:n keys has x:n C 1 children. All leaves are at the same depth in the tree. The lightly shaded nodes\nare examined in a search for the letter R.\nplatter\ntrack\narms\nread/write\nhead\nspindle\nFigure 18.2\nA typical disk drive. It comprises one or more platters (two platters are shown here)",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_7d884b5f-da8a-4599-833b-d34846f9aa05"
                }
            },
            {
                "text": "DELETE. Thus, we can use a search tree both as a dictionary and as a priority\nqueue.\nBasic operations on a binary search tree take time proportional to the height of\nthe tree. For a complete binary tree with n nodes, such operations run in ‚.lg n/\nworst-case time. If the tree is a linear chain of n nodes, however, the same oper-\nations take ‚.n/ worst-case time. We shall see in Section 12.4 that the expected\nheight of a randomly built binary search tree is O.lg n/, so that basic dynamic-set\noperations on such a tree take ‚.lg n/ time on average.\nIn practice, we can’t always guarantee that binary search trees are built ran-\ndomly, but we can design variations of binary search trees with good guaranteed\nworst-case performance on basic operations. Chapter 13 presents one such vari-\nation, red-black trees, which have height O.lg n/. Chapter 18 introduces B-trees,\nwhich are particularly good for maintaining databases on secondary (disk) storage.\nAfter presenting the basic properties of binary search trees, the following sec-\ntions show how to walk a binary search tree to print its values in sorted order, how\nto search for a value in a binary search tree, how to ﬁnd the minimum or maximum\nelement, how to ﬁnd the predecessor or successor of an element, and how to insert\ninto or delete from a binary search tree. The basic mathematical properties of trees\nappear in Appendix B.\n12.1\nWhat is a binary search tree?\nA binary search tree is organized, as the name suggests, in a binary tree, as shown\nin Figure 12.1. We can represent such a tree by a linked data structure in which\neach node is an object. In addition to a key and satellite data, each node contains\nattributes left, right, and p that point to the nodes corresponding to its left child,\n12.1\nWhat is a binary search tree?\n287\n5\n2\n5\n5\n8\n7\n6\n(a)\n6\n8\n7\n5\n2\n(b)\nFigure 12.1\nBinary search trees. For any node x, the keys in the left subtree of x are at most x:key,",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_ca3ab93f-168f-4730-9038-234ded8e94f4"
                }
            },
            {
                "text": "of keys. By taking advantage of these restrictions, they are able to achieve bet-\nter worst-case asymptotic running times than comparison-based data structures.\nFredman and Willard introduced fusion trees [115], which were the ﬁrst data\nstructure to allow faster dictionary operations when the universe is restricted to\nintegers. They showed how to implement these operations in O.lg n= lg lg n/\ntime. Several subsequent data structures, including exponential search trees\n[16], have also given improved bounds on some or all of the dictionary opera-\ntions and are mentioned in the chapter notes throughout this book.\n\u0002\nDynamic graph data structures support various queries while allowing the\nstructure of a graph to change through operations that insert or delete vertices\nor edges. Examples of the queries that they support include vertex connectivity\n[166], edge connectivity, minimum spanning trees [165], biconnectivity, and\ntransitive closure [164].\nChapter notes throughout this book mention additional data structures.\n18\nB-Trees\nB-trees are balanced search trees designed to work well on disks or other direct-\naccess secondary storage devices. B-trees are similar to red-black trees (Chap-\nter 13), but they are better at minimizing disk I/O operations. Many database sys-\ntems use B-trees, or variants of B-trees, to store information.\nB-trees differ from red-black trees in that B-tree nodes may have many children,\nfrom a few to thousands. That is, the “branching factor” of a B-tree can be quite\nlarge, although it usually depends on characteristics of the disk unit used. B-trees\nare similar to red-black trees in that every n-node B-tree has height O.lg n/. The\nexact height of a B-tree can be considerably less than that of a red-black tree,\nhowever, because its branching factor, and hence the base of the logarithm that\nexpresses its height, can be much larger. Therefore, we can also use B-trees to\nimplement many dynamic-set operations in time O.lg n/.",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_e9d9dab3-d2a6-4d4c-88b3-a0f7f6e2f07d"
                }
            },
            {
                "text": "TREE-CREATE, and B-TREE-INSERT. In these procedures, we adopt two con-\nventions:\n\u0002\nThe root of the B-tree is always in main memory, so that we never need to\nperform a DISK-READ on the root; we do have to perform a DISK-WRITE of\nthe root, however, whenever the root node is changed.\n\u0002\nAny nodes that are passed as parameters must already have had a DISK-READ\noperation performed on them.\nThe procedures we present are all “one-pass” algorithms that proceed downward\nfrom the root of the tree, without having to back up.\nSearching a B-tree\nSearching a B-tree is much like searching a binary search tree, except that instead\nof making a binary, or “two-way,” branching decision at each node, we make a\nmultiway branching decision according to the number of the node’s children. More\nprecisely, at each internal node x, we make an .x:n C 1/-way branching decision.\nB-TREE-SEARCH is a straightforward generalization of the TREE-SEARCH pro-\ncedure deﬁned for binary search trees. B-TREE-SEARCH takes as input a pointer\nto the root node x of a subtree and a key k to be searched for in that subtree. The\ntop-level call is thus of the form B-TREE-SEARCH.T:root; k/. If k is in the B-tree,\nB-TREE-SEARCH returns the ordered pair .y; i/ consisting of a node y and an\nindex i such that y:keyi D k. Otherwise, the procedure returns NIL.\n492\nChapter 18\nB-Trees\nB-TREE-SEARCH.x; k/\n1\ni D 1\n2\nwhile i \u0002 x:n and k > x:keyi\n3\ni D i C 1\n4\nif i \u0002 x:n and k == x:keyi\n5\nreturn .x; i/\n6\nelseif x:leaf\n7\nreturn NIL\n8\nelse DISK-READ.x:ci/\n9\nreturn B-TREE-SEARCH.x:ci; k/\nUsing a linear-search procedure, lines 1–3 ﬁnd the smallest index i such that\nk \u0002 x:keyi, or else they set i to x:n C 1. Lines 4–5 check to see whether we\nhave now discovered the key, returning if we have. Otherwise, lines 6–9 either ter-\nminate the search unsuccessfully (if x is a leaf) or recurse to search the appropriate\nsubtree of x, after performing the necessary DISK-READ on that child.",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_d596a7ff-7753-48c7-bf5d-6ed9ac983d34"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nexact height of a B-tree can be considerably less than that of a red-black tree,\nhowever, because its branching factor, and hence the base of the logarithm that\nexpresses its height, can be much larger. Therefore, we can also use B-trees to\nimplement many dynamic-set operations in time O.lg n/.\nB-trees generalize binary search trees in a natural manner. Figure 18.1 shows a\nsimple B-tree. If an internal B-tree node x contains x:n keys, then x has x:n C 1\nchildren. The keys in node x serve as dividing points separating the range of keys\nhandled by x into x:n C 1 subranges, each handled by one child of x. When\nsearching for a key in a B-tree, we make an .x:n C 1/-way decision based on\ncomparisons with the x:n keys stored at node x. The structure of leaf nodes differs\nfrom that of internal nodes; we will examine these differences in Section 18.1.\nSection 18.1 gives a precise deﬁnition of B-trees and proves that the height of\na B-tree grows only logarithmically with the number of nodes it contains. Sec-\ntion 18.2 describes how to search for a key and insert a key into a B-tree, and\nSection 18.3 discusses deletion. Before proceeding, however, we need to ask why\nwe evaluate data structures designed to work on a disk differently from data struc-\ntures designed to work in main random-access memory.\nData structures on secondary storage\nComputer systems take advantage of various technologies that provide memory\ncapacity. The primary memory (or main memory) of a computer system normally\nChapter 18\nB-Trees\n485\nB\nC\nF\nG\nJ\nK\nL\nD H\nN\nP\nR\nS\nV W\nY\nZ\nQ\nT\nX\nM\nT:root\nFigure 18.1\nA B-tree whose keys are the consonants of English. An internal node x containing\nx:n keys has x:n C 1 children. All leaves are at the same depth in the tree. The lightly shaded nodes\nare examined in a search for the letter R.\nplatter\ntrack\narms\nread/write\nhead\nspindle\nFigure 18.2\nA typical disk drive. It comprises one or more platters (two platters are shown here)\n\nDELETE. Thus, we can use a search tree both as a dictionary and as a priority\nqueue.\nBasic operations on a binary search tree take time proportional to the height of\nthe tree. For a complete binary tree with n nodes, such operations run in ‚.lg n/\nworst-case time. If the tree is a linear chain of n nodes, however, the same oper-\nations take ‚.n/ worst-case time. We shall see in Section 12.4 that the expected\nheight of a randomly built binary search tree is O.lg n/, so that basic dynamic-set\noperations on such a tree take ‚.lg n/ time on average.\nIn practice, we can’t always guarantee that binary search trees are built ran-\ndomly, but we can design variations of binary search trees with good guaranteed\nworst-case performance on basic operations. Chapter 13 presents one such vari-\nation, red-black trees, which have height O.lg n/. Chapter 18 introduces B-trees,\nwhich are particularly good for maintaining databases on secondary (disk) storage.\nAfter presenting the basic properties of binary search trees, the following sec-\ntions show how to walk a binary search tree to print its values in sorted order, how\nto search for a value in a binary search tree, how to ﬁnd the minimum or maximum\nelement, how to ﬁnd the predecessor or successor of an element, and how to insert\ninto or delete from a binary search tree. The basic mathematical properties of trees\nappear in Appendix B.\n12.1\nWhat is a binary search tree?\nA binary search tree is organized, as the name suggests, in a binary tree, as shown\nin Figure 12.1. We can represent such a tree by a linked data structure in which\neach node is an object. In addition to a key and satellite data, each node contains\nattributes left, right, and p that point to the nodes corresponding to its left child,\n12.1\nWhat is a binary search tree?\n287\n5\n2\n5\n5\n8\n7\n6\n(a)\n6\n8\n7\n5\n2\n(b)\nFigure 12.1\nBinary search trees. For any node x, the keys in the left subtree of x are at most x:key,\n\nof keys. By taking advantage of these restrictions, they are able to achieve bet-\nter worst-case asymptotic running times than comparison-based data structures.\nFredman and Willard introduced fusion trees [115], which were the ﬁrst data\nstructure to allow faster dictionary operations when the universe is restricted to\nintegers. They showed how to implement these operations in O.lg n= lg lg n/\ntime. Several subsequent data structures, including exponential search trees\n[16], have also given improved bounds on some or all of the dictionary opera-\ntions and are mentioned in the chapter notes throughout this book.\n\u0002\nDynamic graph data structures support various queries while allowing the\nstructure of a graph to change through operations that insert or delete vertices\nor edges. Examples of the queries that they support include vertex connectivity\n[166], edge connectivity, minimum spanning trees [165], biconnectivity, and\ntransitive closure [164].\nChapter notes throughout this book mention additional data structures.\n18\nB-Trees\nB-trees are balanced search trees designed to work well on disks or other direct-\naccess secondary storage devices. B-trees are similar to red-black trees (Chap-\nter 13), but they are better at minimizing disk I/O operations. Many database sys-\ntems use B-trees, or variants of B-trees, to store information.\nB-trees differ from red-black trees in that B-tree nodes may have many children,\nfrom a few to thousands. That is, the “branching factor” of a B-tree can be quite\nlarge, although it usually depends on characteristics of the disk unit used. B-trees\nare similar to red-black trees in that every n-node B-tree has height O.lg n/. The\nexact height of a B-tree can be considerably less than that of a red-black tree,\nhowever, because its branching factor, and hence the base of the logarithm that\nexpresses its height, can be much larger. Therefore, we can also use B-trees to\nimplement many dynamic-set operations in time O.lg n/.\n\nTREE-CREATE, and B-TREE-INSERT. In these procedures, we adopt two con-\nventions:\n\u0002\nThe root of the B-tree is always in main memory, so that we never need to\nperform a DISK-READ on the root; we do have to perform a DISK-WRITE of\nthe root, however, whenever the root node is changed.\n\u0002\nAny nodes that are passed as parameters must already have had a DISK-READ\noperation performed on them.\nThe procedures we present are all “one-pass” algorithms that proceed downward\nfrom the root of the tree, without having to back up.\nSearching a B-tree\nSearching a B-tree is much like searching a binary search tree, except that instead\nof making a binary, or “two-way,” branching decision at each node, we make a\nmultiway branching decision according to the number of the node’s children. More\nprecisely, at each internal node x, we make an .x:n C 1/-way branching decision.\nB-TREE-SEARCH is a straightforward generalization of the TREE-SEARCH pro-\ncedure deﬁned for binary search trees. B-TREE-SEARCH takes as input a pointer\nto the root node x of a subtree and a key k to be searched for in that subtree. The\ntop-level call is thus of the form B-TREE-SEARCH.T:root; k/. If k is in the B-tree,\nB-TREE-SEARCH returns the ordered pair .y; i/ consisting of a node y and an\nindex i such that y:keyi D k. Otherwise, the procedure returns NIL.\n492\nChapter 18\nB-Trees\nB-TREE-SEARCH.x; k/\n1\ni D 1\n2\nwhile i \u0002 x:n and k > x:keyi\n3\ni D i C 1\n4\nif i \u0002 x:n and k == x:keyi\n5\nreturn .x; i/\n6\nelseif x:leaf\n7\nreturn NIL\n8\nelse DISK-READ.x:ci/\n9\nreturn B-TREE-SEARCH.x:ci; k/\nUsing a linear-search procedure, lines 1–3 ﬁnd the smallest index i such that\nk \u0002 x:keyi, or else they set i to x:n C 1. Lines 4–5 check to see whether we\nhave now discovered the key, returning if we have. Otherwise, lines 6–9 either ter-\nminate the search unsuccessfully (if x is a leaf) or recurse to search the appropriate\nsubtree of x, after performing the necessary DISK-READ on that child.\n\n問題：From the book's perspective, what are the fundamental structural and performance differences between a standard binary search tree and a B-tree? Why are B-trees preferred for disk-based storage systems?\n\n回答：",
        "llm_final_answer": "根據提供的資料，以下是標準二元搜尋樹和 B-樹之間的基本結構和效能差異，以及 B-樹在基於磁碟的儲存系統中更受青睞的原因：\n\n*   **結構差異：**\n    *   **二元搜尋樹：** 每個節點最多有兩個子節點（左子節點和右子節點）。\n    *   **B-樹：** 節點可以有多個子節點。內部節點 x 包含 x:n 個鍵，則 x 具有 x:n+1 個子節點。B-樹的「分支因子」可以很大。\n*   **效能差異：**\n    *   **高度：**  二元搜尋樹的高度取決於建構方式。  B-樹的高度為 O(lg n)，其中 n 是節點的數量。\n    *   **搜尋時間：** B-樹可用於在 O(lg n) 時間內實現許多動態集合操作。\n*   **為何 B-樹更適合基於磁碟的儲存系統：**\n    *   **最小化磁碟 I/O 操作：** B-樹專為與磁碟或其他直接存取輔助儲存裝置配合使用而設計，且更擅長於最小化磁碟 I/O 操作。\n    *   **分支因子：** B-樹的分支因子可以非常大，這意味著 B-樹的高度可以遠小於紅黑樹的高度，進而減少存取時間。",
        "error_message": null,
        "question_id": "EN_BST_BTREE_001"
    },
    {
        "original_question": "請描述「快速排序 (Quicksort)」演算法的核心思想，特別是其 PARTITION (分割) 程序。另外，它的最壞情況 (worst-case) 時間複雜度是多少？在什麼情況下會發生？",
        "decomposed_queries_text": "1.  什麼是快速排序演算法的核心思想？\n2.  PARTITION (分割) 程序在快速排序演算法中扮演什麼角色？\n3.  請詳細描述快速排序演算法的 PARTITION (分割) 程序是如何運作的？\n4.  快速排序演算法的最壞情況 (worst-case) 時間複雜度是多少？\n5.  什麼情況下會導致快速排序演算法達到最壞情況的時間複雜度？",
        "hypothetical_document": "快速排序是一種基於比較且採用分治法的排序演算法，其核心思想是透過 PARTITION 程序，選擇一個樞紐元素 (pivot)，將陣列分割成兩部分：小於樞紐的元素和大於樞紐的元素。 PARTITION 程序通常以 `O(n)` 時間複雜度執行，完成樞紐元素的定位。快速排序的最壞情況時間複雜度為 `O(n^2)`，發生於樞紐元素選擇不當，例如每次選擇陣列中最大或最小的元素。",
        "query_for_retrieval": "快速排序是一種基於比較且採用分治法的排序演算法，其核心思想是透過 PARTITION 程序，選擇一個樞紐元素 (pivot)，將陣列分割成兩部分：小於樞紐的元素和大於樞紐的元素。 PARTITION 程序通常以 `O(n)` 時間複雜度執行，完成樞紐元素的定位。快速排序的最壞情況時間複雜度為 `O(n^2)`，發生於樞紐元素選擇不當，例如每次選擇陣列中最大或最小的元素。",
        "retrieved_contexts_from_server": [
            {
                "text": "as insertion sort. In this section, we shall informally investigate how quicksort\nperforms under the assumptions of balanced versus unbalanced partitioning.\nWorst-case partitioning\nThe worst-case behavior for quicksort occurs when the partitioning routine pro-\nduces one subproblem with n \u0005 1 elements and one with 0 elements. (We prove\nthis claim in Section 7.4.1.) Let us assume that this unbalanced partitioning arises\nin each recursive call. The partitioning costs ‚.n/ time. Since the recursive call\non an array of size 0 just returns, T .0/ D ‚.1/, and the recurrence for the running\ntime is\nT .n/\nD\nT .n \u0005 1/ C T .0/ C ‚.n/\nD\nT .n \u0005 1/ C ‚.n/ :\nIntuitively, if we sum the costs incurred at each level of the recursion, we get\nan arithmetic series (equation (A.2)), which evaluates to ‚.n2/.\nIndeed, it is\nstraightforward to use the substitution method to prove that the recurrence T .n/ D\nT .n \u0005 1/ C ‚.n/ has the solution T .n/ D ‚.n2/. (See Exercise 7.2-1.)\nThus, if the partitioning is maximally unbalanced at every recursive level of the\nalgorithm, the running time is ‚.n2/. Therefore the worst-case running time of\nquicksort is no better than that of insertion sort. Moreover, the ‚.n2/ running time\noccurs when the input array is already completely sorted—a common situation in\nwhich insertion sort runs in O.n/ time.\nBest-case partitioning\nIn the most even possible split, PARTITION produces two subproblems, each of\nsize no more than n=2, since one is of size bn=2c and one of size dn=2e\u00051. In this\ncase, quicksort runs much faster. The recurrence for the running time is then\nT .n/ D 2T .n=2/ C ‚.n/ ;\nwhere we tolerate the sloppiness from ignoring the ﬂoor and ceiling and from sub-\ntracting 1. By case 2 of the master theorem (Theorem 4.1), this recurrence has the\nsolution T .n/ D ‚.n lg n/. By equally balancing the two sides of the partition at\nevery level of the recursion, we get an asymptotically faster algorithm.\nBalanced partitioning",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_eb020da6-55c5-4677-a8ba-c860fea36865"
                }
            },
            {
                "text": "7.3\nA randomized version of quicksort\n179\n7.2-6\n?\nArgue that for any constant 0 < ˛ \u0002 1=2, the probability is approximately 1 \u0005 2˛\nthat on a random input array, PARTITION produces a split more balanced than 1\u0005˛\nto ˛.\n7.3\nA randomized version of quicksort\nIn exploring the average-case behavior of quicksort, we have made an assumption\nthat all permutations of the input numbers are equally likely. In an engineering\nsituation, however, we cannot always expect this assumption to hold. (See Exer-\ncise 7.2-4.) As we saw in Section 5.3, we can sometimes add randomization to an\nalgorithm in order to obtain good expected performance over all inputs. Many peo-\nple regard the resulting randomized version of quicksort as the sorting algorithm\nof choice for large enough inputs.\nIn Section 5.3, we randomized our algorithm by explicitly permuting the in-\nput. We could do so for quicksort also, but a different randomization technique,\ncalled random sampling, yields a simpler analysis. Instead of always using AŒr\u0002\nas the pivot, we will select a randomly chosen element from the subarray AŒp : : r\u0002.\nWe do so by ﬁrst exchanging element AŒr\u0002 with an element chosen at random\nfrom AŒp : : r\u0002. By randomly sampling the range p; : : : ; r, we ensure that the pivot\nelement x D AŒr\u0002 is equally likely to be any of the r \u0005 p C 1 elements in the\nsubarray. Because we randomly choose the pivot element, we expect the split of\nthe input array to be reasonably well balanced on average.\nThe changes to PARTITION and QUICKSORT are small. In the new partition\nprocedure, we simply implement the swap before actually partitioning:\nRANDOMIZED-PARTITION.A; p; r/\n1\ni D RANDOM.p; r/\n2\nexchange AŒr\u0002 with AŒi\u0002\n3\nreturn PARTITION.A; p; r/\nThe new quicksort calls RANDOMIZED-PARTITION in place of PARTITION:\nRANDOMIZED-QUICKSORT.A; p; r/\n1\nif p < r\n2\nq D RANDOMIZED-PARTITION.A; p; r/\n3\nRANDOMIZED-QUICKSORT.A; p; q \u0005 1/\n4\nRANDOMIZED-QUICKSORT.A; q C 1; r/\nWe analyze this algorithm in the next section.\n180\nChapter 7",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_855efa1e-935c-4df2-b620-108f3a71c1fc"
                }
            },
            {
                "text": "bound we saw in Section 7.2, yields a ‚.n lg n/ expected running time. We assume\nthroughout that the values of the elements being sorted are distinct.\nRunning time and comparisons\nThe QUICKSORT and RANDOMIZED-QUICKSORT procedures differ only in how\nthey select pivot elements; they are the same in all other respects. We can therefore\ncouch our analysis of RANDOMIZED-QUICKSORT by discussing the QUICKSORT\nand PARTITION procedures, but with the assumption that pivot elements are se-\nlected randomly from the subarray passed to RANDOMIZED-PARTITION.\nThe running time of QUICKSORT is dominated by the time spent in the PARTI-\nTION procedure. Each time the PARTITION procedure is called, it selects a pivot\nelement, and this element is never included in any future recursive calls to QUICK-\nSORT and PARTITION. Thus, there can be at most n calls to PARTITION over the\nentire execution of the quicksort algorithm. One call to PARTITION takes O.1/\ntime plus an amount of time that is proportional to the number of iterations of the\nfor loop in lines 3–6. Each iteration of this for loop performs a comparison in\nline 4, comparing the pivot element to another element of the array A. Therefore,\n182\nChapter 7\nQuicksort\nif we can count the total number of times that line 4 is executed, we can bound the\ntotal time spent in the for loop during the entire execution of QUICKSORT.\nLemma 7.1\nLet X be the number of comparisons performed in line 4 of PARTITION over the\nentire execution of QUICKSORT on an n-element array. Then the running time of\nQUICKSORT is O.n C X/.\nProof\nBy the discussion above, the algorithm makes at most n calls to PARTI-\nTION, each of which does a constant amount of work and then executes the for\nloop some number of times. Each iteration of the for loop executes line 4.\nOur goal, therefore, is to compute X, the total number of comparisons performed\nin all calls to PARTITION. We will not attempt to analyze how many comparisons",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_d6257791-57c5-4c8d-8877-b57e86f90707"
                }
            },
            {
                "text": "when HOARE-PARTITION terminates.\nThe PARTITION procedure in Section 7.1 separates the pivot value (originally\nin AŒr\u0002) from the two partitions it forms. The HOARE-PARTITION procedure, on\nthe other hand, always places the pivot value (originally in AŒp\u0002) into one of the\ntwo partitions AŒp : : j \u0002 and AŒj C 1 : : r\u0002. Since p \u0002 j < r, this split is always\nnontrivial.\ne. Rewrite the QUICKSORT procedure to use HOARE-PARTITION.\n7-2\nQuicksort with equal element values\nThe analysis of the expected running time of randomized quicksort in Section 7.4.2\nassumes that all element values are distinct. In this problem, we examine what\nhappens when they are not.\na. Suppose that all element values are equal. What would be randomized quick-\nsort’s running time in this case?\nb. The PARTITION procedure returns an index q such that each element of\nAŒp : : q \u0005 1\u0002 is less than or equal to AŒq\u0002 and each element of AŒq C 1 : : r\u0002\nis greater than AŒq\u0002. Modify the PARTITION procedure to produce a procedure\nPARTITION0.A; p; r/, which permutes the elements of AŒp : : r\u0002 and returns two\nindices q and t, where p \u0002 q \u0002 t \u0002 r, such that\n\u0002 all elements of AŒq : : t\u0002 are equal,\n\u0002 each element of AŒp : : q \u0005 1\u0002 is less than AŒq\u0002, and\n\u0002 each element of AŒt C 1 : : r\u0002 is greater than AŒq\u0002.\nLike PARTITION, your PARTITION0 procedure should take ‚.r \u0005 p/ time.\nc. Modify the RANDOMIZED-QUICKSORT procedure to call PARTITION0, and\nname the new procedure RANDOMIZED-QUICKSORT0.\nThen modify the\nQUICKSORT procedure to produce a procedure QUICKSORT0.p; r/ that calls\nProblems for Chapter 7\n187\nRANDOMIZED-PARTITION0 and recurses only on partitions of elements not\nknown to be equal to each other.\nd. Using QUICKSORT0, how would you adjust the analysis in Section 7.4.2 to\navoid the assumption that all elements are distinct?\n7-3\nAlternative quicksort analysis\nAn alternative analysis of the running time of randomized quicksort focuses on\nthe expected running time of each individual recursive call to RANDOMIZED-",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_89dba089-1692-4bcb-8cca-5d0a5c25970a"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nas insertion sort. In this section, we shall informally investigate how quicksort\nperforms under the assumptions of balanced versus unbalanced partitioning.\nWorst-case partitioning\nThe worst-case behavior for quicksort occurs when the partitioning routine pro-\nduces one subproblem with n \u0005 1 elements and one with 0 elements. (We prove\nthis claim in Section 7.4.1.) Let us assume that this unbalanced partitioning arises\nin each recursive call. The partitioning costs ‚.n/ time. Since the recursive call\non an array of size 0 just returns, T .0/ D ‚.1/, and the recurrence for the running\ntime is\nT .n/\nD\nT .n \u0005 1/ C T .0/ C ‚.n/\nD\nT .n \u0005 1/ C ‚.n/ :\nIntuitively, if we sum the costs incurred at each level of the recursion, we get\nan arithmetic series (equation (A.2)), which evaluates to ‚.n2/.\nIndeed, it is\nstraightforward to use the substitution method to prove that the recurrence T .n/ D\nT .n \u0005 1/ C ‚.n/ has the solution T .n/ D ‚.n2/. (See Exercise 7.2-1.)\nThus, if the partitioning is maximally unbalanced at every recursive level of the\nalgorithm, the running time is ‚.n2/. Therefore the worst-case running time of\nquicksort is no better than that of insertion sort. Moreover, the ‚.n2/ running time\noccurs when the input array is already completely sorted—a common situation in\nwhich insertion sort runs in O.n/ time.\nBest-case partitioning\nIn the most even possible split, PARTITION produces two subproblems, each of\nsize no more than n=2, since one is of size bn=2c and one of size dn=2e\u00051. In this\ncase, quicksort runs much faster. The recurrence for the running time is then\nT .n/ D 2T .n=2/ C ‚.n/ ;\nwhere we tolerate the sloppiness from ignoring the ﬂoor and ceiling and from sub-\ntracting 1. By case 2 of the master theorem (Theorem 4.1), this recurrence has the\nsolution T .n/ D ‚.n lg n/. By equally balancing the two sides of the partition at\nevery level of the recursion, we get an asymptotically faster algorithm.\nBalanced partitioning\n\n7.3\nA randomized version of quicksort\n179\n7.2-6\n?\nArgue that for any constant 0 < ˛ \u0002 1=2, the probability is approximately 1 \u0005 2˛\nthat on a random input array, PARTITION produces a split more balanced than 1\u0005˛\nto ˛.\n7.3\nA randomized version of quicksort\nIn exploring the average-case behavior of quicksort, we have made an assumption\nthat all permutations of the input numbers are equally likely. In an engineering\nsituation, however, we cannot always expect this assumption to hold. (See Exer-\ncise 7.2-4.) As we saw in Section 5.3, we can sometimes add randomization to an\nalgorithm in order to obtain good expected performance over all inputs. Many peo-\nple regard the resulting randomized version of quicksort as the sorting algorithm\nof choice for large enough inputs.\nIn Section 5.3, we randomized our algorithm by explicitly permuting the in-\nput. We could do so for quicksort also, but a different randomization technique,\ncalled random sampling, yields a simpler analysis. Instead of always using AŒr\u0002\nas the pivot, we will select a randomly chosen element from the subarray AŒp : : r\u0002.\nWe do so by ﬁrst exchanging element AŒr\u0002 with an element chosen at random\nfrom AŒp : : r\u0002. By randomly sampling the range p; : : : ; r, we ensure that the pivot\nelement x D AŒr\u0002 is equally likely to be any of the r \u0005 p C 1 elements in the\nsubarray. Because we randomly choose the pivot element, we expect the split of\nthe input array to be reasonably well balanced on average.\nThe changes to PARTITION and QUICKSORT are small. In the new partition\nprocedure, we simply implement the swap before actually partitioning:\nRANDOMIZED-PARTITION.A; p; r/\n1\ni D RANDOM.p; r/\n2\nexchange AŒr\u0002 with AŒi\u0002\n3\nreturn PARTITION.A; p; r/\nThe new quicksort calls RANDOMIZED-PARTITION in place of PARTITION:\nRANDOMIZED-QUICKSORT.A; p; r/\n1\nif p < r\n2\nq D RANDOMIZED-PARTITION.A; p; r/\n3\nRANDOMIZED-QUICKSORT.A; p; q \u0005 1/\n4\nRANDOMIZED-QUICKSORT.A; q C 1; r/\nWe analyze this algorithm in the next section.\n180\nChapter 7\n\nbound we saw in Section 7.2, yields a ‚.n lg n/ expected running time. We assume\nthroughout that the values of the elements being sorted are distinct.\nRunning time and comparisons\nThe QUICKSORT and RANDOMIZED-QUICKSORT procedures differ only in how\nthey select pivot elements; they are the same in all other respects. We can therefore\ncouch our analysis of RANDOMIZED-QUICKSORT by discussing the QUICKSORT\nand PARTITION procedures, but with the assumption that pivot elements are se-\nlected randomly from the subarray passed to RANDOMIZED-PARTITION.\nThe running time of QUICKSORT is dominated by the time spent in the PARTI-\nTION procedure. Each time the PARTITION procedure is called, it selects a pivot\nelement, and this element is never included in any future recursive calls to QUICK-\nSORT and PARTITION. Thus, there can be at most n calls to PARTITION over the\nentire execution of the quicksort algorithm. One call to PARTITION takes O.1/\ntime plus an amount of time that is proportional to the number of iterations of the\nfor loop in lines 3–6. Each iteration of this for loop performs a comparison in\nline 4, comparing the pivot element to another element of the array A. Therefore,\n182\nChapter 7\nQuicksort\nif we can count the total number of times that line 4 is executed, we can bound the\ntotal time spent in the for loop during the entire execution of QUICKSORT.\nLemma 7.1\nLet X be the number of comparisons performed in line 4 of PARTITION over the\nentire execution of QUICKSORT on an n-element array. Then the running time of\nQUICKSORT is O.n C X/.\nProof\nBy the discussion above, the algorithm makes at most n calls to PARTI-\nTION, each of which does a constant amount of work and then executes the for\nloop some number of times. Each iteration of the for loop executes line 4.\nOur goal, therefore, is to compute X, the total number of comparisons performed\nin all calls to PARTITION. We will not attempt to analyze how many comparisons\n\nwhen HOARE-PARTITION terminates.\nThe PARTITION procedure in Section 7.1 separates the pivot value (originally\nin AŒr\u0002) from the two partitions it forms. The HOARE-PARTITION procedure, on\nthe other hand, always places the pivot value (originally in AŒp\u0002) into one of the\ntwo partitions AŒp : : j \u0002 and AŒj C 1 : : r\u0002. Since p \u0002 j < r, this split is always\nnontrivial.\ne. Rewrite the QUICKSORT procedure to use HOARE-PARTITION.\n7-2\nQuicksort with equal element values\nThe analysis of the expected running time of randomized quicksort in Section 7.4.2\nassumes that all element values are distinct. In this problem, we examine what\nhappens when they are not.\na. Suppose that all element values are equal. What would be randomized quick-\nsort’s running time in this case?\nb. The PARTITION procedure returns an index q such that each element of\nAŒp : : q \u0005 1\u0002 is less than or equal to AŒq\u0002 and each element of AŒq C 1 : : r\u0002\nis greater than AŒq\u0002. Modify the PARTITION procedure to produce a procedure\nPARTITION0.A; p; r/, which permutes the elements of AŒp : : r\u0002 and returns two\nindices q and t, where p \u0002 q \u0002 t \u0002 r, such that\n\u0002 all elements of AŒq : : t\u0002 are equal,\n\u0002 each element of AŒp : : q \u0005 1\u0002 is less than AŒq\u0002, and\n\u0002 each element of AŒt C 1 : : r\u0002 is greater than AŒq\u0002.\nLike PARTITION, your PARTITION0 procedure should take ‚.r \u0005 p/ time.\nc. Modify the RANDOMIZED-QUICKSORT procedure to call PARTITION0, and\nname the new procedure RANDOMIZED-QUICKSORT0.\nThen modify the\nQUICKSORT procedure to produce a procedure QUICKSORT0.p; r/ that calls\nProblems for Chapter 7\n187\nRANDOMIZED-PARTITION0 and recurses only on partitions of elements not\nknown to be equal to each other.\nd. Using QUICKSORT0, how would you adjust the analysis in Section 7.4.2 to\navoid the assumption that all elements are distinct?\n7-3\nAlternative quicksort analysis\nAn alternative analysis of the running time of randomized quicksort focuses on\nthe expected running time of each individual recursive call to RANDOMIZED-\n\n問題：請描述「快速排序 (Quicksort)」演算法的核心思想，特別是其 PARTITION (分割) 程序。另外，它的最壞情況 (worst-case) 時間複雜度是多少？在什麼情況下會發生？\n\n回答：",
        "llm_final_answer": "快速排序 (Quicksort) 演算法的核心思想是**分治法 (Divide and Conquer)**。它通過以下步驟來排序一個數組：\n\n1.  **分割 (Partition)：**  選擇一個元素作為“樞軸 (pivot)”。重新排列數組，使得所有小於樞軸的元素都移到樞軸的左邊，所有大於樞軸的元素都移到樞軸的右邊。樞軸現在處於它在已排序數組中的最終位置。\n\n2.  **遞迴 (Recursion)：**  遞迴地將分割後的子數組 (即樞軸左邊和右邊的子數組) 分別進行快速排序。\n\n**PARTITION (分割) 程序**：\n\n*   在分割程序中，選擇一個樞軸元素 (例如，數組的最後一個元素)。\n*   重新排列數組，使得所有小於樞軸的元素都移到樞軸的左邊，所有大於樞軸的元素都移到樞軸的右邊。\n*   分割程序的核心工作就是確定樞軸最終的位置，並將數組分成兩部分。\n\n**最壞情況 (Worst-case) 時間複雜度：**\n\n快速排序的最壞情況時間複雜度是 **O(n²)**。\n\n**發生最壞情況的情況：**\n\n當分割程序每次產生一個大小為 n-1 的子問題和一個大小為 0 的子問題時，就會發生最壞情況。這通常發生在以下情況：\n\n*   輸入數組已經完全排序 (升序或降序)。",
        "error_message": null,
        "question_id": "ZH_QUICKSORT_001"
    },
    {
        "original_question": "The concept of \"optimal substructure\" is critical for dynamic programming. Explain this property in detail and provide an example of a problem that exhibits it, as discussed in the book.",
        "decomposed_queries_text": "1.  什麼是「最佳子結構」？\n2.  「最佳子結構」在動態規劃中的重要性是什麼？\n3.  提供一個展示「最佳子結構」的經典問題範例。\n4.  詳細解釋所選問題如何展示「最佳子結構」特性。",
        "hypothetical_document": "Optimal substructure, a core tenet of dynamic programming, dictates that an optimal solution to a problem can be constructed from optimal solutions to its subproblems. This property allows for the recursive decomposition of the problem into smaller, overlapping subproblems, enabling efficient solutions. The matrix chain multiplication problem, with its `O(n^3)` runtime complexity utilizing dynamic programming, perfectly illustrates this principle by building an optimal parenthesization through the optimal solutions of multiplying smaller matrix chains.",
        "query_for_retrieval": "Optimal substructure, a core tenet of dynamic programming, dictates that an optimal solution to a problem can be constructed from optimal solutions to its subproblems. This property allows for the recursive decomposition of the problem into smaller, overlapping subproblems, enabling efficient solutions. The matrix chain multiplication problem, with its `O(n^3)` runtime complexity utilizing dynamic programming, perfectly illustrates this principle by building an optimal parenthesization through the optimal solutions of multiplying smaller matrix chains.",
        "retrieved_contexts_from_server": [
            {
                "text": "We can usually make the greedy choice more efﬁciently than when we have to\nconsider a wider set of choices. For example, in the activity-selection problem, as-\n16.2\nElements of the greedy strategy\n425\nsuming that we had already sorted the activities in monotonically increasing order\nof ﬁnish times, we needed to examine each activity just once. By preprocessing the\ninput or by using an appropriate data structure (often a priority queue), we often\ncan make greedy choices quickly, thus yielding an efﬁcient algorithm.\nOptimal substructure\nA problem exhibits optimal substructure if an optimal solution to the problem\ncontains within it optimal solutions to subproblems. This property is a key in-\ngredient of assessing the applicability of dynamic programming as well as greedy\nalgorithms. As an example of optimal substructure, recall how we demonstrated in\nSection 16.1 that if an optimal solution to subproblem Sij includes an activity ak,\nthen it must also contain optimal solutions to the subproblems Sik and Skj . Given\nthis optimal substructure, we argued that if we knew which activity to use as ak, we\ncould construct an optimal solution to Sij by selecting ak along with all activities\nin optimal solutions to the subproblems Sik and Skj . Based on this observation of\noptimal substructure, we were able to devise the recurrence (16.2) that described\nthe value of an optimal solution.\nWe usually use a more direct approach regarding optimal substructure when\napplying it to greedy algorithms. As mentioned above, we have the luxury of\nassuming that we arrived at a subproblem by having made the greedy choice in\nthe original problem. All we really need to do is argue that an optimal solution to\nthe subproblem, combined with the greedy choice already made, yields an optimal\nsolution to the original problem. This scheme implicitly uses induction on the\nsubproblems to prove that making the greedy choice at every step produces an\noptimal solution.\nGreedy versus dynamic programming",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_e9923fa3-59ae-443b-a321-06cebc43d20f"
                }
            },
            {
                "text": "X\niD1\nn\nX\njDi\nR.i; j / D n3 \u0005 n\n3\n:\n(Hint: You may ﬁnd equation (A.3) useful.)\n15.2-6\nShow that a full parenthesization of an n-element expression has exactly n\u00051 pairs\nof parentheses.\n15.3\nElements of dynamic programming\nAlthough we have just worked through two examples of the dynamic-programming\nmethod, you might still be wondering just when the method applies. From an en-\ngineering perspective, when should we look for a dynamic-programming solution\nto a problem? In this section, we examine the two key ingredients that an opti-\n15.3\nElements of dynamic programming\n379\nmization problem must have in order for dynamic programming to apply: optimal\nsubstructure and overlapping subproblems. We also revisit and discuss more fully\nhow memoization might help us take advantage of the overlapping-subproblems\nproperty in a top-down recursive approach.\nOptimal substructure\nThe ﬁrst step in solving an optimization problem by dynamic programming is to\ncharacterize the structure of an optimal solution. Recall that a problem exhibits\noptimal substructure if an optimal solution to the problem contains within it opti-\nmal solutions to subproblems. Whenever a problem exhibits optimal substructure,\nwe have a good clue that dynamic programming might apply. (As Chapter 16 dis-\ncusses, it also might mean that a greedy strategy applies, however.) In dynamic\nprogramming, we build an optimal solution to the problem from optimal solutions\nto subproblems. Consequently, we must take care to ensure that the range of sub-\nproblems we consider includes those used in an optimal solution.\nWe discovered optimal substructure in both of the problems we have examined\nin this chapter so far. In Section 15.1, we observed that the optimal way of cut-\nting up a rod of length n (if we make any cuts at all) involves optimally cutting\nup the two pieces resulting from the ﬁrst cut. In Section 15.2, we observed that\nan optimal parenthesization of AiAiC1 \u0003 \u0003 \u0003 Aj that splits the product between Ak",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_f2e41328-4f30-43bf-b37f-6214bd20b458"
                }
            },
            {
                "text": "uct, we have two subproblems—parenthesizing AiAiC1 \u0003 \u0003 \u0003 Ak and parenthesizing\nAkC1AkC2 \u0003 \u0003 \u0003 Aj—and we must solve both of them optimally. Once we determine\nthe optimal solutions to subproblems, we choose from among j \u0005 i candidates for\nthe index k.\nInformally, the running time of a dynamic-programming algorithm depends on\nthe product of two factors: the number of subproblems overall and how many\nchoices we look at for each subproblem. In rod cutting, we had ‚.n/ subproblems\noverall, and at most n choices to examine for each, yielding an O.n2/ running time.\nMatrix-chain multiplication had ‚.n2/ subproblems overall, and in each we had at\nmost n \u0005 1 choices, giving an O.n3/ running time (actually, a ‚.n3/ running time,\nby Exercise 15.2-5).\nUsually, the subproblem graph gives an alternative way to perform the same\nanalysis. Each vertex corresponds to a subproblem, and the choices for a sub-\n15.3\nElements of dynamic programming\n381\nproblem are the edges incident to that subproblem. Recall that in rod cutting,\nthe subproblem graph had n vertices and at most n edges per vertex, yielding an\nO.n2/ running time. For matrix-chain multiplication, if we were to draw the sub-\nproblem graph, it would have ‚.n2/ vertices and each vertex would have degree at\nmost n \u0005 1, giving a total of O.n3/ vertices and edges.\nDynamic programming often uses optimal substructure in a bottom-up fashion.\nThat is, we ﬁrst ﬁnd optimal solutions to subproblems and, having solved the sub-\nproblems, we ﬁnd an optimal solution to the problem. Finding an optimal solu-\ntion to the problem entails making a choice among subproblems as to which we\nwill use in solving the problem. The cost of the problem solution is usually the\nsubproblem costs plus a cost that is directly attributable to the choice itself. In\nrod cutting, for example, ﬁrst we solved the subproblems of determining optimal\nways to cut up rods of length i for i D 0; 1; : : : ; n \u0005 1, and then we determined",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_7ef28edb-22e0-4a18-9a7a-51b0c9639c9a"
                }
            },
            {
                "text": "this part will recur later in this book.\nDynamic programming typically applies to optimization problems in which we\nmake a set of choices in order to arrive at an optimal solution.\nAs we make\neach choice, subproblems of the same form often arise. Dynamic programming\nis effective when a given subproblem may arise from more than one partial set of\nchoices; the key technique is to store the solution to each such subproblem in case it\nshould reappear. Chapter 15 shows how this simple idea can sometimes transform\nexponential-time algorithms into polynomial-time algorithms.\nLike dynamic-programming algorithms, greedy algorithms typically apply to\noptimization problems in which we make a set of choices in order to arrive at an\noptimal solution. The idea of a greedy algorithm is to make each choice in a locally\noptimal manner. A simple example is coin-changing: to minimize the number of\nU.S. coins needed to make change for a given amount, we can repeatedly select\nthe largest-denomination coin that is not larger than the amount that remains. A\ngreedy approach provides an optimal solution for many such problems much more\nquickly than would a dynamic-programming approach. We cannot always easily\ntell whether a greedy approach will be effective, however. Chapter 16 introduces\n358\nPart IV\nAdvanced Design and Analysis Techniques\nmatroid theory, which provides a mathematical basis that can help us to show that\na greedy algorithm yields an optimal solution.\nWe use amortized analysis to analyze certain algorithms that perform a sequence\nof similar operations. Instead of bounding the cost of the sequence of operations\nby bounding the actual cost of each operation separately, an amortized analysis\nprovides a bound on the actual cost of the entire sequence. One advantage of this\napproach is that although some operations might be expensive, many others might\nbe cheap. In other words, many of the operations might run in well under the worst-",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_822a2955-b082-4c97-81cc-df2561774672"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nWe can usually make the greedy choice more efﬁciently than when we have to\nconsider a wider set of choices. For example, in the activity-selection problem, as-\n16.2\nElements of the greedy strategy\n425\nsuming that we had already sorted the activities in monotonically increasing order\nof ﬁnish times, we needed to examine each activity just once. By preprocessing the\ninput or by using an appropriate data structure (often a priority queue), we often\ncan make greedy choices quickly, thus yielding an efﬁcient algorithm.\nOptimal substructure\nA problem exhibits optimal substructure if an optimal solution to the problem\ncontains within it optimal solutions to subproblems. This property is a key in-\ngredient of assessing the applicability of dynamic programming as well as greedy\nalgorithms. As an example of optimal substructure, recall how we demonstrated in\nSection 16.1 that if an optimal solution to subproblem Sij includes an activity ak,\nthen it must also contain optimal solutions to the subproblems Sik and Skj . Given\nthis optimal substructure, we argued that if we knew which activity to use as ak, we\ncould construct an optimal solution to Sij by selecting ak along with all activities\nin optimal solutions to the subproblems Sik and Skj . Based on this observation of\noptimal substructure, we were able to devise the recurrence (16.2) that described\nthe value of an optimal solution.\nWe usually use a more direct approach regarding optimal substructure when\napplying it to greedy algorithms. As mentioned above, we have the luxury of\nassuming that we arrived at a subproblem by having made the greedy choice in\nthe original problem. All we really need to do is argue that an optimal solution to\nthe subproblem, combined with the greedy choice already made, yields an optimal\nsolution to the original problem. This scheme implicitly uses induction on the\nsubproblems to prove that making the greedy choice at every step produces an\noptimal solution.\nGreedy versus dynamic programming\n\nX\niD1\nn\nX\njDi\nR.i; j / D n3 \u0005 n\n3\n:\n(Hint: You may ﬁnd equation (A.3) useful.)\n15.2-6\nShow that a full parenthesization of an n-element expression has exactly n\u00051 pairs\nof parentheses.\n15.3\nElements of dynamic programming\nAlthough we have just worked through two examples of the dynamic-programming\nmethod, you might still be wondering just when the method applies. From an en-\ngineering perspective, when should we look for a dynamic-programming solution\nto a problem? In this section, we examine the two key ingredients that an opti-\n15.3\nElements of dynamic programming\n379\nmization problem must have in order for dynamic programming to apply: optimal\nsubstructure and overlapping subproblems. We also revisit and discuss more fully\nhow memoization might help us take advantage of the overlapping-subproblems\nproperty in a top-down recursive approach.\nOptimal substructure\nThe ﬁrst step in solving an optimization problem by dynamic programming is to\ncharacterize the structure of an optimal solution. Recall that a problem exhibits\noptimal substructure if an optimal solution to the problem contains within it opti-\nmal solutions to subproblems. Whenever a problem exhibits optimal substructure,\nwe have a good clue that dynamic programming might apply. (As Chapter 16 dis-\ncusses, it also might mean that a greedy strategy applies, however.) In dynamic\nprogramming, we build an optimal solution to the problem from optimal solutions\nto subproblems. Consequently, we must take care to ensure that the range of sub-\nproblems we consider includes those used in an optimal solution.\nWe discovered optimal substructure in both of the problems we have examined\nin this chapter so far. In Section 15.1, we observed that the optimal way of cut-\nting up a rod of length n (if we make any cuts at all) involves optimally cutting\nup the two pieces resulting from the ﬁrst cut. In Section 15.2, we observed that\nan optimal parenthesization of AiAiC1 \u0003 \u0003 \u0003 Aj that splits the product between Ak\n\nuct, we have two subproblems—parenthesizing AiAiC1 \u0003 \u0003 \u0003 Ak and parenthesizing\nAkC1AkC2 \u0003 \u0003 \u0003 Aj—and we must solve both of them optimally. Once we determine\nthe optimal solutions to subproblems, we choose from among j \u0005 i candidates for\nthe index k.\nInformally, the running time of a dynamic-programming algorithm depends on\nthe product of two factors: the number of subproblems overall and how many\nchoices we look at for each subproblem. In rod cutting, we had ‚.n/ subproblems\noverall, and at most n choices to examine for each, yielding an O.n2/ running time.\nMatrix-chain multiplication had ‚.n2/ subproblems overall, and in each we had at\nmost n \u0005 1 choices, giving an O.n3/ running time (actually, a ‚.n3/ running time,\nby Exercise 15.2-5).\nUsually, the subproblem graph gives an alternative way to perform the same\nanalysis. Each vertex corresponds to a subproblem, and the choices for a sub-\n15.3\nElements of dynamic programming\n381\nproblem are the edges incident to that subproblem. Recall that in rod cutting,\nthe subproblem graph had n vertices and at most n edges per vertex, yielding an\nO.n2/ running time. For matrix-chain multiplication, if we were to draw the sub-\nproblem graph, it would have ‚.n2/ vertices and each vertex would have degree at\nmost n \u0005 1, giving a total of O.n3/ vertices and edges.\nDynamic programming often uses optimal substructure in a bottom-up fashion.\nThat is, we ﬁrst ﬁnd optimal solutions to subproblems and, having solved the sub-\nproblems, we ﬁnd an optimal solution to the problem. Finding an optimal solu-\ntion to the problem entails making a choice among subproblems as to which we\nwill use in solving the problem. The cost of the problem solution is usually the\nsubproblem costs plus a cost that is directly attributable to the choice itself. In\nrod cutting, for example, ﬁrst we solved the subproblems of determining optimal\nways to cut up rods of length i for i D 0; 1; : : : ; n \u0005 1, and then we determined\n\nthis part will recur later in this book.\nDynamic programming typically applies to optimization problems in which we\nmake a set of choices in order to arrive at an optimal solution.\nAs we make\neach choice, subproblems of the same form often arise. Dynamic programming\nis effective when a given subproblem may arise from more than one partial set of\nchoices; the key technique is to store the solution to each such subproblem in case it\nshould reappear. Chapter 15 shows how this simple idea can sometimes transform\nexponential-time algorithms into polynomial-time algorithms.\nLike dynamic-programming algorithms, greedy algorithms typically apply to\noptimization problems in which we make a set of choices in order to arrive at an\noptimal solution. The idea of a greedy algorithm is to make each choice in a locally\noptimal manner. A simple example is coin-changing: to minimize the number of\nU.S. coins needed to make change for a given amount, we can repeatedly select\nthe largest-denomination coin that is not larger than the amount that remains. A\ngreedy approach provides an optimal solution for many such problems much more\nquickly than would a dynamic-programming approach. We cannot always easily\ntell whether a greedy approach will be effective, however. Chapter 16 introduces\n358\nPart IV\nAdvanced Design and Analysis Techniques\nmatroid theory, which provides a mathematical basis that can help us to show that\na greedy algorithm yields an optimal solution.\nWe use amortized analysis to analyze certain algorithms that perform a sequence\nof similar operations. Instead of bounding the cost of the sequence of operations\nby bounding the actual cost of each operation separately, an amortized analysis\nprovides a bound on the actual cost of the entire sequence. One advantage of this\napproach is that although some operations might be expensive, many others might\nbe cheap. In other words, many of the operations might run in well under the worst-\n\n問題：The concept of \"optimal substructure\" is critical for dynamic programming. Explain this property in detail and provide an example of a problem that exhibits it, as discussed in the book.\n\n回答：",
        "llm_final_answer": "動態規劃的核心概念之一是「最佳子結構」。\n\n最佳子結構指的是，一個問題的最佳解包含了其子問題的最佳解。 換句話說，解決一個問題的最佳方法，會用到其子問題的最佳解。\n\n根據提供的資料，動態規劃通常用於解決最佳化問題，而最佳子結構是動態規劃是否適用的關鍵指標。\n\n**例子：**\n\n書中提到了兩個例子，它們都展現了最佳子結構：\n\n*   **切鋼條問題（Rod cutting）：** 如果要以最佳方式切斷長度為 n 的鋼條（如果要做任何切割），則需要以最佳方式切斷第一次切割產生的兩個片段。\n\n*   **矩陣鏈乘法（Matrix-chain multiplication）：** 如果 AiAi+1...Aj 的最佳加括弧方式在 Ak 處分割乘積，那麼它必須包含 AiAi+1...Ak 和 Ak+1Ak+2...Aj 的最佳加括弧方式。\n\n也就是說，在解決這些問題時，需要先找到子問題的最佳解，然後基於這些子問題的最佳解來構建原問題的最佳解。",
        "error_message": null,
        "question_id": "EN_DP_OPTIMAL_SUBSTRUCTURE_001"
    },
    {
        "original_question": "書中如何介紹解決「最大流問題 (maximum-flow problem)」的 Ford-Fulkerson 方法？請說明其核心概念，特別是「殘餘網路 (residual network)」和「增廣路徑 (augmenting path)」所扮演的角色。",
        "decomposed_queries_text": "1.  Ford-Fulkerson 方法是什麼？\n2.  Ford-Fulkerson 方法解決的最大流問題是什麼？\n3.  Ford-Fulkerson 方法的核心概念是什麼？\n4.  「殘餘網路 (residual network)」是什麼？它在 Ford-Fulkerson 方法中扮演什麼角色？\n5.  「增廣路徑 (augmenting path)」是什麼？它在 Ford-Fulkerson 方法中扮演什麼角色？\n6.  Ford-Fulkerson 方法如何運作？",
        "hypothetical_document": "Ford-Fulkerson 方法透過迭代搜尋增廣路徑來解決最大流問題。其核心概念是構建殘餘網路，該網路顯示了邊緣上的剩餘容量和反向邊緣，允許取消先前分配的流量。算法反覆尋找殘餘網路中的增廣路徑，沿著這些路徑推送流量，直到無法找到更多增廣路徑，此時達到了最大流，其時間複雜度為 O(E * f)，其中 E 是邊數，f 是最大流的值。",
        "query_for_retrieval": "Ford-Fulkerson 方法透過迭代搜尋增廣路徑來解決最大流問題。其核心概念是構建殘餘網路，該網路顯示了邊緣上的剩餘容量和反向邊緣，允許取消先前分配的流量。算法反覆尋找殘餘網路中的增廣路徑，沿著這些路徑推送流量，直到無法找到更多增廣路徑，此時達到了最大流，其時間複雜度為 O(E * f)，其中 E 是邊數，f 是最大流的值。",
        "retrieved_contexts_from_server": [
            {
                "text": "26.1-5\nState the maximum-ﬂow problem as a linear-programming problem.\n26.1-6\nProfessor Adam has two children who, unfortunately, dislike each other. The prob-\nlem is so severe that not only do they refuse to walk to school together, but in fact\neach one refuses to walk on any block that the other child has stepped on that day.\nThe children have no problem with their paths crossing at a corner. Fortunately\nboth the professor’s house and the school are on corners, but beyond that he is not\nsure if it is going to be possible to send both of his children to the same school.\nThe professor has a map of his town. Show how to formulate the problem of de-\ntermining whether both his children can go to the same school as a maximum-ﬂow\nproblem.\n26.1-7\nSuppose that, in addition to edge capacities, a ﬂow network has vertex capacities.\nThat is each vertex \u0003 has a limit l.\u0003/ on how much ﬂow can pass though \u0003. Show\nhow to transform a ﬂow network G D .V; E/ with vertex capacities into an equiv-\nalent ﬂow network G0 D .V 0; E0/ without vertex capacities, such that a maximum\nﬂow in G0 has the same value as a maximum ﬂow in G. How many vertices and\nedges does G0 have?\n26.2\nThe Ford-Fulkerson method\nThis section presents the Ford-Fulkerson method for solving the maximum-ﬂow\nproblem. We call it a “method” rather than an “algorithm” because it encompasses\nseveral implementations with differing running times. The Ford-Fulkerson method\ndepends on three important ideas that transcend the method and are relevant to\nmany ﬂow algorithms and problems: residual networks, augmenting paths, and\ncuts. These ideas are essential to the important max-ﬂow min-cut theorem (The-\norem 26.6), which characterizes the value of a maximum ﬂow in terms of cuts of\n26.2\nThe Ford-Fulkerson method\n715\nthe ﬂow network. We end this section by presenting one speciﬁc implementation\nof the Ford-Fulkerson method and analyzing its running time.\nThe Ford-Fulkerson method iteratively increases the value of the ﬂow. We start",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_2e7be80e-f264-44b6-8b1b-2f2fa13b578d"
                }
            },
            {
                "text": "original network or the reversal of an edge in the original network. Lines 6–8\nupdate the ﬂow in each case appropriately, adding ﬂow when the residual edge is\nan original edge and subtracting it otherwise. When no augmenting paths exist, the\nﬂow f is a maximum ﬂow.\nAnalysis of Ford-Fulkerson\nThe running time of FORD-FULKERSON depends on how we ﬁnd the augmenting\npath p in line 3. If we choose it poorly, the algorithm might not even terminate: the\nvalue of the ﬂow will increase with successive augmentations, but it need not even\nconverge to the maximum ﬂow value.2 If we ﬁnd the augmenting path by using a\nbreadth-ﬁrst search (which we saw in Section 22.2), however, the algorithm runs in\npolynomial time. Before proving this result, we obtain a simple bound for the case\nin which we choose the augmenting path arbitrarily and all capacities are integers.\nIn practice, the maximum-ﬂow problem often arises with integral capacities. If\nthe capacities are rational numbers, we can apply an appropriate scaling transfor-\nmation to make them all integral. If f \u0004 denotes a maximum ﬂow in the transformed\nnetwork, then a straightforward implementation of FORD-FULKERSON executes\nthe while loop of lines 3–8 at most jf \u0004j times, since the ﬂow value increases by at\nleast one unit in each iteration.\nWe can perform the work done within the while loop efﬁciently if we implement\nthe ﬂow network G D .V; E/ with the right data structure and ﬁnd an augmenting\npath by a linear-time algorithm. Let us assume that we keep a data structure cor-\nresponding to a directed graph G0 D .V; E0/, where E0 D f.u; \u0003/ W .u; \u0003/ 2 E or\n.\u0003; u/ 2 Eg. Edges in the network G are also edges in G0, and therefore we can\neasily maintain capacities and ﬂows in this data structure. Given a ﬂow f on G,\nthe edges in the residual network Gf consist of all edges .u; \u0003/ of G0 such that\ncf .u; \u0003/ > 0, where cf conforms to equation (26.2). The time to ﬁnd a path in",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_2a4dba39-8cb0-4bbf-9f1c-03c1cbdaf4d0"
                }
            },
            {
                "text": "p\njV j?)\ng. Give an algorithm that runs in O.E/ time to ﬁnd a maximal set of vertex-\ndisjoint shortest augmenting paths P1; P2; : : : ; Pk for a given matching M.\nConclude that the total running time of HOPCROFT-KARP is O.\np\nV E/.\nChapter notes\nAhuja, Magnanti, and Orlin [7], Even [103], Lawler [224], Papadimitriou and Stei-\nglitz [271], and Tarjan [330] are good references for network ﬂow and related algo-\nrithms. Goldberg, Tardos, and Tarjan [139] also provide a nice survey of algorithms\nfor network-ﬂow problems, and Schrijver [304] has written an interesting review\nof historical developments in the ﬁeld of network ﬂows.\nThe Ford-Fulkerson method is due to Ford and Fulkerson [109], who originated\nthe formal study of many of the problems in the area of network ﬂow, including\nthe maximum-ﬂow and bipartite-matching problems. Many early implementations\nof the Ford-Fulkerson method found augmenting paths using breadth-ﬁrst search;\nEdmonds and Karp [102], and independently Dinic [89], proved that this strategy\nyields a polynomial-time algorithm. A related idea, that of using “blocking ﬂows,”\nwas also ﬁrst developed by Dinic [89]. Karzanov [202] ﬁrst developed the idea of\npreﬂows. The push-relabel method is due to Goldberg [136] and Goldberg and Tar-\njan [140]. Goldberg and Tarjan gave an O.V 3/-time algorithm that uses a queue to\nmaintain the set of overﬂowing vertices, as well as an algorithm that uses dynamic\ntrees to achieve a running time of O.VE lg.V 2=E C2//. Several other researchers\nhave developed push-relabel maximum-ﬂow algorithms. Ahuja and Orlin [9] and\nAhuja, Orlin, and Tarjan [10] gave algorithms that used scaling. Cheriyan and\nMaheshwari [62] proposed pushing ﬂow from the overﬂowing vertex of maximum\nheight. Cheriyan and Hagerup [61] suggested randomly permuting the neighbor\nlists, and several researchers [14, 204, 276] developed clever derandomizations of\nthis idea, leading to a sequence of faster algorithms. The algorithm of King, Rao,",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_19b251ce-ab9c-41e3-a54c-fe1aeda0d7d0"
                }
            },
            {
                "text": "tion (26.7).\n26.2-2\nIn Figure 26.1(b), what is the ﬂow across the cut .fs; \u00032; \u00034g ; f\u00031; \u00033; tg/? What is\nthe capacity of this cut?\n26.2-3\nShow the execution of the Edmonds-Karp algorithm on the ﬂow network of Fig-\nure 26.1(a).\n26.2-4\nIn the example of Figure 26.6, what is the minimum cut corresponding to the max-\nimum ﬂow shown? Of the augmenting paths appearing in the example, which one\ncancels ﬂow?\n26.2-5\nRecall that the construction in Section 26.1 that converts a ﬂow network with mul-\ntiple sources and sinks into a single-source, single-sink network adds edges with\ninﬁnite capacity. Prove that any ﬂow in the resulting network has a ﬁnite value\nif the edges of the original network with multiple sources and sinks have ﬁnite\ncapacity.\n26.2-6\nSuppose that each source si in a ﬂow network with multiple sources and sinks\nproduces exactly pi units of ﬂow, so that P\n\u00052V f .si; \u0003/ D pi. Suppose also\nthat each sink tj consumes exactly qj units, so that P\n\u00052V f .\u0003; tj/ D qj, where\nP\ni pi D P\nj qj. Show how to convert the problem of ﬁnding a ﬂow f that obeys\n26.2\nThe Ford-Fulkerson method\n731\nthese additional constraints into the problem of ﬁnding a maximum ﬂow in a single-\nsource, single-sink ﬂow network.\n26.2-7\nProve Lemma 26.2.\n26.2-8\nSuppose that we redeﬁne the residual network to disallow edges into s. Argue that\nthe procedure FORD-FULKERSON still correctly computes a maximum ﬂow.\n26.2-9\nSuppose that both f and f 0 are ﬂows in a network G and we compute ﬂow f \" f 0.\nDoes the augmented ﬂow satisfy the ﬂow conservation property? Does it satisfy\nthe capacity constraint?\n26.2-10\nShow how to ﬁnd a maximum ﬂow in a network G D .V; E/ by a sequence of at\nmost jEj augmenting paths. (Hint: Determine the paths after ﬁnding the maximum\nﬂow.)\n26.2-11\nThe edge connectivity of an undirected graph is the minimum number k of edges\nthat must be removed to disconnect the graph. For example, the edge connectivity",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_d18a8114-0b78-440f-8619-60774b331242"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\n26.1-5\nState the maximum-ﬂow problem as a linear-programming problem.\n26.1-6\nProfessor Adam has two children who, unfortunately, dislike each other. The prob-\nlem is so severe that not only do they refuse to walk to school together, but in fact\neach one refuses to walk on any block that the other child has stepped on that day.\nThe children have no problem with their paths crossing at a corner. Fortunately\nboth the professor’s house and the school are on corners, but beyond that he is not\nsure if it is going to be possible to send both of his children to the same school.\nThe professor has a map of his town. Show how to formulate the problem of de-\ntermining whether both his children can go to the same school as a maximum-ﬂow\nproblem.\n26.1-7\nSuppose that, in addition to edge capacities, a ﬂow network has vertex capacities.\nThat is each vertex \u0003 has a limit l.\u0003/ on how much ﬂow can pass though \u0003. Show\nhow to transform a ﬂow network G D .V; E/ with vertex capacities into an equiv-\nalent ﬂow network G0 D .V 0; E0/ without vertex capacities, such that a maximum\nﬂow in G0 has the same value as a maximum ﬂow in G. How many vertices and\nedges does G0 have?\n26.2\nThe Ford-Fulkerson method\nThis section presents the Ford-Fulkerson method for solving the maximum-ﬂow\nproblem. We call it a “method” rather than an “algorithm” because it encompasses\nseveral implementations with differing running times. The Ford-Fulkerson method\ndepends on three important ideas that transcend the method and are relevant to\nmany ﬂow algorithms and problems: residual networks, augmenting paths, and\ncuts. These ideas are essential to the important max-ﬂow min-cut theorem (The-\norem 26.6), which characterizes the value of a maximum ﬂow in terms of cuts of\n26.2\nThe Ford-Fulkerson method\n715\nthe ﬂow network. We end this section by presenting one speciﬁc implementation\nof the Ford-Fulkerson method and analyzing its running time.\nThe Ford-Fulkerson method iteratively increases the value of the ﬂow. We start\n\noriginal network or the reversal of an edge in the original network. Lines 6–8\nupdate the ﬂow in each case appropriately, adding ﬂow when the residual edge is\nan original edge and subtracting it otherwise. When no augmenting paths exist, the\nﬂow f is a maximum ﬂow.\nAnalysis of Ford-Fulkerson\nThe running time of FORD-FULKERSON depends on how we ﬁnd the augmenting\npath p in line 3. If we choose it poorly, the algorithm might not even terminate: the\nvalue of the ﬂow will increase with successive augmentations, but it need not even\nconverge to the maximum ﬂow value.2 If we ﬁnd the augmenting path by using a\nbreadth-ﬁrst search (which we saw in Section 22.2), however, the algorithm runs in\npolynomial time. Before proving this result, we obtain a simple bound for the case\nin which we choose the augmenting path arbitrarily and all capacities are integers.\nIn practice, the maximum-ﬂow problem often arises with integral capacities. If\nthe capacities are rational numbers, we can apply an appropriate scaling transfor-\nmation to make them all integral. If f \u0004 denotes a maximum ﬂow in the transformed\nnetwork, then a straightforward implementation of FORD-FULKERSON executes\nthe while loop of lines 3–8 at most jf \u0004j times, since the ﬂow value increases by at\nleast one unit in each iteration.\nWe can perform the work done within the while loop efﬁciently if we implement\nthe ﬂow network G D .V; E/ with the right data structure and ﬁnd an augmenting\npath by a linear-time algorithm. Let us assume that we keep a data structure cor-\nresponding to a directed graph G0 D .V; E0/, where E0 D f.u; \u0003/ W .u; \u0003/ 2 E or\n.\u0003; u/ 2 Eg. Edges in the network G are also edges in G0, and therefore we can\neasily maintain capacities and ﬂows in this data structure. Given a ﬂow f on G,\nthe edges in the residual network Gf consist of all edges .u; \u0003/ of G0 such that\ncf .u; \u0003/ > 0, where cf conforms to equation (26.2). The time to ﬁnd a path in\n\np\njV j?)\ng. Give an algorithm that runs in O.E/ time to ﬁnd a maximal set of vertex-\ndisjoint shortest augmenting paths P1; P2; : : : ; Pk for a given matching M.\nConclude that the total running time of HOPCROFT-KARP is O.\np\nV E/.\nChapter notes\nAhuja, Magnanti, and Orlin [7], Even [103], Lawler [224], Papadimitriou and Stei-\nglitz [271], and Tarjan [330] are good references for network ﬂow and related algo-\nrithms. Goldberg, Tardos, and Tarjan [139] also provide a nice survey of algorithms\nfor network-ﬂow problems, and Schrijver [304] has written an interesting review\nof historical developments in the ﬁeld of network ﬂows.\nThe Ford-Fulkerson method is due to Ford and Fulkerson [109], who originated\nthe formal study of many of the problems in the area of network ﬂow, including\nthe maximum-ﬂow and bipartite-matching problems. Many early implementations\nof the Ford-Fulkerson method found augmenting paths using breadth-ﬁrst search;\nEdmonds and Karp [102], and independently Dinic [89], proved that this strategy\nyields a polynomial-time algorithm. A related idea, that of using “blocking ﬂows,”\nwas also ﬁrst developed by Dinic [89]. Karzanov [202] ﬁrst developed the idea of\npreﬂows. The push-relabel method is due to Goldberg [136] and Goldberg and Tar-\njan [140]. Goldberg and Tarjan gave an O.V 3/-time algorithm that uses a queue to\nmaintain the set of overﬂowing vertices, as well as an algorithm that uses dynamic\ntrees to achieve a running time of O.VE lg.V 2=E C2//. Several other researchers\nhave developed push-relabel maximum-ﬂow algorithms. Ahuja and Orlin [9] and\nAhuja, Orlin, and Tarjan [10] gave algorithms that used scaling. Cheriyan and\nMaheshwari [62] proposed pushing ﬂow from the overﬂowing vertex of maximum\nheight. Cheriyan and Hagerup [61] suggested randomly permuting the neighbor\nlists, and several researchers [14, 204, 276] developed clever derandomizations of\nthis idea, leading to a sequence of faster algorithms. The algorithm of King, Rao,\n\ntion (26.7).\n26.2-2\nIn Figure 26.1(b), what is the ﬂow across the cut .fs; \u00032; \u00034g ; f\u00031; \u00033; tg/? What is\nthe capacity of this cut?\n26.2-3\nShow the execution of the Edmonds-Karp algorithm on the ﬂow network of Fig-\nure 26.1(a).\n26.2-4\nIn the example of Figure 26.6, what is the minimum cut corresponding to the max-\nimum ﬂow shown? Of the augmenting paths appearing in the example, which one\ncancels ﬂow?\n26.2-5\nRecall that the construction in Section 26.1 that converts a ﬂow network with mul-\ntiple sources and sinks into a single-source, single-sink network adds edges with\ninﬁnite capacity. Prove that any ﬂow in the resulting network has a ﬁnite value\nif the edges of the original network with multiple sources and sinks have ﬁnite\ncapacity.\n26.2-6\nSuppose that each source si in a ﬂow network with multiple sources and sinks\nproduces exactly pi units of ﬂow, so that P\n\u00052V f .si; \u0003/ D pi. Suppose also\nthat each sink tj consumes exactly qj units, so that P\n\u00052V f .\u0003; tj/ D qj, where\nP\ni pi D P\nj qj. Show how to convert the problem of ﬁnding a ﬂow f that obeys\n26.2\nThe Ford-Fulkerson method\n731\nthese additional constraints into the problem of ﬁnding a maximum ﬂow in a single-\nsource, single-sink ﬂow network.\n26.2-7\nProve Lemma 26.2.\n26.2-8\nSuppose that we redeﬁne the residual network to disallow edges into s. Argue that\nthe procedure FORD-FULKERSON still correctly computes a maximum ﬂow.\n26.2-9\nSuppose that both f and f 0 are ﬂows in a network G and we compute ﬂow f \" f 0.\nDoes the augmented ﬂow satisfy the ﬂow conservation property? Does it satisfy\nthe capacity constraint?\n26.2-10\nShow how to ﬁnd a maximum ﬂow in a network G D .V; E/ by a sequence of at\nmost jEj augmenting paths. (Hint: Determine the paths after ﬁnding the maximum\nﬂow.)\n26.2-11\nThe edge connectivity of an undirected graph is the minimum number k of edges\nthat must be removed to disconnect the graph. For example, the edge connectivity\n\n問題：書中如何介紹解決「最大流問題 (maximum-flow problem)」的 Ford-Fulkerson 方法？請說明其核心概念，特別是「殘餘網路 (residual network)」和「增廣路徑 (augmenting path)」所扮演的角色。\n\n回答：",
        "llm_final_answer": "Ford-Fulkerson 方法是一種解決最大流問題的方法，它通過迭代地增加流的值來找到最大流。\n\n以下是根據提供的資料對 Ford-Fulkerson 方法的解釋：\n\n*   **核心概念：**\n    *   **迭代增加流量：** Ford-Fulkerson 方法通過不斷增加網絡中的流量來工作。\n    *   **停止條件：** 當找不到增廣路徑時，演算法結束，此時的流就是最大流。\n*   **核心概念：**\n    *   **殘餘網絡 (residual network):** 殘餘網絡表示在給定的流情況下，還可以在網絡中增加多少流量。殘餘網絡的邊包括原始網絡中的邊（如果它們還有剩餘容量）和原始網絡中反向邊（表示可以減少流）。\n    *   **增廣路徑 (augmenting path):** 增廣路徑是在殘餘網絡中從源點到匯點的一條路徑。沿着增廣路徑，可以增加流量。\n*   **Ford-Fulkerson 方法的步驟：**\n    1.  從一個初始流開始，通常是零流。\n    2.  在殘餘網絡中尋找一條增廣路徑。\n    3.  沿着增廣路徑增加流量。\n    4.  重複步驟 2 和 3，直到殘餘網絡中不再存在增廣路徑。\n*   **重要性：** Ford-Fulkerson 方法依賴於三個重要的概念：殘餘網絡、增廣路徑和割。這些概念對於許多流演算法和問題至關重要，並且與重要的最大流最小割定理相關。\n\n總之，Ford-Fulkerson 方法通過在殘餘網絡中尋找增廣路徑，並沿着這些路徑增加流量，從而迭代地改進流，直到達到最大流。殘餘網絡和增廣路徑是理解該方法的核心。",
        "error_message": null,
        "question_id": "ZH_MAXFLOW_FORD_FULKERSON_001"
    },
    {
        "original_question": "Using the dynamic table expansion problem as an example, explain how the \"potential method\" of amortized analysis works. Define the potential function and show how it pays for expensive operations.",
        "decomposed_queries_text": "1.  什麼是動態表格擴展問題？\n2.  什麼是攤銷分析？\n3.  攤銷分析中的潛能方法是什麼？\n4.  潛能函數是什麼？\n5.  在動態表格擴展問題中，如何定義潛能函數？\n6.  潛能函數如何支付昂貴的操作？\n7.  提供動態表格擴展問題中使用潛能方法的範例。",
        "hypothetical_document": "在動態表格擴展問題的背景下，攤銷分析的勢能法利用勢能函數 Φ 描述系統的「能量」。每次操作的攤銷成本定義為實際成本加上勢能的變化，即 `amortized cost = actual cost + ΔΦ`。當勢能增加時，預先支付了後續昂貴操作的成本；當勢能減少時，為當前操作釋放了勢能。例如，在表格擴展中，可定義 Φ 為表格的已使用元素數量乘以一個常數，由此可以證明，擴展操作的攤銷成本保持在 `O(1)`。",
        "query_for_retrieval": "在動態表格擴展問題的背景下，攤銷分析的勢能法利用勢能函數 Φ 描述系統的「能量」。每次操作的攤銷成本定義為實際成本加上勢能的變化，即 `amortized cost = actual cost + ΔΦ`。當勢能增加時，預先支付了後續昂貴操作的成本；當勢能減少時，為當前操作釋放了勢能。例如，在表格擴展中，可定義 Φ 為表格的已使用元素數量乘以一個常數，由此可以證明，擴展操作的攤銷成本保持在 `O(1)`。",
        "retrieved_contexts_from_server": [
            {
                "text": "factor is 1=2, the potential is 0. When the load factor is 1, we have T:size D T:num,\nwhich implies ˆ.T / D T:num, and thus the potential can pay for an expansion if\nan item is inserted. When the load factor is 1=4, we have T:size D 4\u0003T:num, which\n470\nChapter 17\nAmortized Analysis\nimplies ˆ.T / D T:num, and thus the potential can pay for a contraction if an item\nis deleted.\nTo analyze a sequence of n TABLE-INSERT and TABLE-DELETE operations,\nwe let ci denote the actual cost of the ith operation, yci denote its amortized cost\nwith respect to ˆ, numi denote the number of items stored in the table after the ith\noperation, sizei denote the total size of the table after the ith operation, ˛i denote\nthe load factor of the table after the ith operation, and ˆi denote the potential after\nthe ith operation. Initially, num0 D 0, size0 D 0, ˛0 D 1, and ˆ0 D 0.\nWe start with the case in which the ith operation is TABLE-INSERT. The analy-\nsis is identical to that for table expansion in Section 17.4.1 if ˛i\u00031 \u0006 1=2. Whether\nthe table expands or not, the amortized cost yci of the operation is at most 3.\nIf ˛i\u00031 < 1=2, the table cannot expand as a result of the operation, since the ta-\nble expands only when ˛i\u00031 D 1. If ˛i < 1=2 as well, then the amortized cost of\nthe ith operation is\nyci\nD\nci C ˆi \u0005 ˆi\u00031\nD\n1 C .sizei=2 \u0005 numi/ \u0005 .sizei\u00031=2 \u0005 numi\u00031/\nD\n1 C .sizei=2 \u0005 numi/ \u0005 .sizei=2 \u0005 .numi \u0005 1//\nD\n0 :\nIf ˛i\u00031 < 1=2 but ˛i \u0006 1=2, then\nyci\nD\nci C ˆi \u0005 ˆi\u00031\nD\n1 C .2 \u0003 numi \u0005 sizei/ \u0005 .sizei\u00031=2 \u0005 numi\u00031/\nD\n1 C .2.numi\u00031 C 1/ \u0005 sizei\u00031/ \u0005 .sizei\u00031=2 \u0005 numi\u00031/\nD\n3 \u0003 numi\u00031 \u0005 3\n2sizei\u00031 C 3\nD\n3˛i\u00031sizei\u00031 \u0005 3\n2sizei\u00031 C 3\n<\n3\n2sizei\u00031 \u0005 3\n2sizei\u00031 C 3\nD\n3 :\nThus, the amortized cost of a TABLE-INSERT operation is at most 3.\nWe now turn to the case in which the ith operation is TABLE-DELETE. In this\ncase, numi D numi\u00031 \u0005 1. If ˛i\u00031 < 1=2, then we must consider whether the\noperation causes the table to contract. If it does not, then sizei D sizei\u00031 and the",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_6a769039-e474-482e-a4ff-3a9bdb6693c7"
                }
            },
            {
                "text": "expansion, and so forth. The cost of each expansion and contraction is ‚.n/, and\nthere are ‚.n/ of them. Thus, the total cost of the n operations is ‚.n2/, making\nthe amortized cost of an operation ‚.n/.\nThe downside of this strategy is obvious: after expanding the table, we do not\ndelete enough items to pay for a contraction. Likewise, after contracting the table,\nwe do not insert enough items to pay for an expansion.\nWe can improve upon this strategy by allowing the load factor of the table to\ndrop below 1=2. Speciﬁcally, we continue to double the table size upon inserting\nan item into a full table, but we halve the table size when deleting an item causes\nthe table to become less than 1=4 full, rather than 1=2 full as before. The load\nfactor of the table is therefore bounded below by the constant 1=4.\nIntuitively, we would consider a load factor of 1=2 to be ideal, and the table’s\npotential would then be 0. As the load factor deviates from 1=2, the potential\nincreases so that by the time we expand or contract the table, the table has garnered\nsufﬁcient potential to pay for copying all the items into the newly allocated table.\nThus, we will need a potential function that has grown to T:num by the time that\nthe load factor has either increased to 1 or decreased to 1=4. After either expanding\nor contracting the table, the load factor goes back to 1=2 and the table’s potential\nreduces back to 0.\nWe omit the code for TABLE-DELETE, since it is analogous to TABLE-INSERT.\nFor our analysis, we shall assume that whenever the number of items in the table\ndrops to 0, we free the storage for the table. That is, if T:num D 0, then T:size D 0.\nWe can now use the potential method to analyze the cost of a sequence of n\nTABLE-INSERT and TABLE-DELETE operations. We start by deﬁning a poten-\ntial function ˆ that is 0 immediately after an expansion or contraction and builds\nas the load factor increases to 1 or decreases to 1=4. Let us denote the load fac-\n17.4\nDynamic tables\n469\nnumi",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_96c3ab79-1694-4c03-8d13-befdf2745217"
                }
            },
            {
                "text": "immediately after an expansion. Then the table holds m=2 items, and it contains\n466\nChapter 17\nAmortized Analysis\nno credit. We charge 3 dollars for each insertion. The elementary insertion that\noccurs immediately costs 1 dollar. We place another dollar as credit on the item\ninserted. We place the third dollar as credit on one of the m=2 items already in the\ntable. The table will not ﬁll again until we have inserted another m=2 \u0005 1 items,\nand thus, by the time the table contains m items and is full, we will have placed a\ndollar on each item to pay to reinsert it during the expansion.\nWe can use the potential method to analyze a sequence of n TABLE-INSERT\noperations, and we shall use it in Section 17.4.2 to design a TABLE-DELETE op-\neration that has an O.1/ amortized cost as well. We start by deﬁning a potential\nfunction ˆ that is 0 immediately after an expansion but builds to the table size by\nthe time the table is full, so that we can pay for the next expansion by the potential.\nThe function\nˆ.T / D 2 \u0003 T:num \u0005 T:size\n(17.5)\nis one possibility. Immediately after an expansion, we have T:num D T:size=2,\nand thus ˆ.T / D 0, as desired.\nImmediately before an expansion, we have\nT:num D T:size, and thus ˆ.T / D T:num, as desired. The initial value of the\npotential is 0, and since the table is always at least half full, T:num \u0006 T:size=2,\nwhich implies that ˆ.T / is always nonnegative. Thus, the sum of the amortized\ncosts of n TABLE-INSERT operations gives an upper bound on the sum of the actual\ncosts.\nTo analyze the amortized cost of the ith TABLE-INSERT operation, we let numi\ndenote the number of items stored in the table after the ith operation, sizei denote\nthe total size of the table after the ith operation, and ˆi denote the potential after\nthe ith operation. Initially, we have num0 D 0, size0 D 0, and ˆ0 D 0.\nIf the ith TABLE-INSERT operation does not trigger an expansion, then we have\nsizei D sizei\u00031 and the amortized cost of the operation is\nyci\nD\nci C ˆi \u0005 ˆi\u00031",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_dad4c2ff-cac2-4a15-8a6c-23f5fd6b531c"
                }
            },
            {
                "text": "that ˆ.Di/ \u0006 ˆ.D0/, the total amortized cost of n operations is an upper bound\non the total actual cost. The worst-case cost of n operations is therefore O.n/.\nIncrementing a binary counter\nAs another example of the potential method, we again look at incrementing a binary\ncounter. This time, we deﬁne the potential of the counter after the ith INCREMENT\noperation to be bi, the number of 1s in the counter after the ith operation.\nLet us compute the amortized cost of an INCREMENT operation. Suppose that\nthe ith INCREMENT operation resets ti bits. The actual cost of the operation is\ntherefore at most ti C 1, since in addition to resetting ti bits, it sets at most one\nbit to 1. If bi D 0, then the ith operation resets all k bits, and so bi\u00031 D ti D k.\nIf bi > 0, then bi D bi\u00031 \u0005 ti C 1. In either case, bi \u0002 bi\u00031 \u0005 ti C 1, and the\npotential difference is\nˆ.Di/ \u0005 ˆ.Di\u00031/\n\u0002\n.bi\u00031 \u0005 ti C 1/ \u0005 bi\u00031\nD\n1 \u0005 ti :\nThe amortized cost is therefore\nyci\nD\nci C ˆ.Di/ \u0005 ˆ.Di\u00031/\n\u0002\n.ti C 1/ C .1 \u0005 ti/\nD\n2 :\nIf the counter starts at zero, then ˆ.D0/ D 0. Since ˆ.Di/ \u0006 0 for all i, the total\namortized cost of a sequence of n INCREMENT operations is an upper bound on the\ntotal actual cost, and so the worst-case cost of n INCREMENT operations is O.n/.\nThe potential method gives us an easy way to analyze the counter even when\nit does not start at zero. The counter starts with b0 1s, and after n INCREMENT\n462\nChapter 17\nAmortized Analysis\noperations it has bn 1s, where 0 \u0002 b0; bn \u0002 k. (Recall that k is the number of bits\nin the counter.) We can rewrite equation (17.3) as\nn\nX\niD1\nci D\nn\nX\niD1\nyci \u0005 ˆ.Dn/ C ˆ.D0/ :\n(17.4)\nWe have yci \u0002 2 for all 1 \u0002 i \u0002 n. Since ˆ.D0/ D b0 and ˆ.Dn/ D bn, the total\nactual cost of n INCREMENT operations is\nn\nX\niD1\nci\n\u0002\nn\nX\niD1\n2 \u0005 bn C b0\nD\n2n \u0005 bn C b0 :\nNote in particular that since b0 \u0002 k, as long as k D O.n/, the total actual cost\nis O.n/. In other words, if we execute at least n D \u0004.k/ INCREMENT operations,",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_d9fa73c8-dab4-49ac-ac2b-9c628ff36620"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nfactor is 1=2, the potential is 0. When the load factor is 1, we have T:size D T:num,\nwhich implies ˆ.T / D T:num, and thus the potential can pay for an expansion if\nan item is inserted. When the load factor is 1=4, we have T:size D 4\u0003T:num, which\n470\nChapter 17\nAmortized Analysis\nimplies ˆ.T / D T:num, and thus the potential can pay for a contraction if an item\nis deleted.\nTo analyze a sequence of n TABLE-INSERT and TABLE-DELETE operations,\nwe let ci denote the actual cost of the ith operation, yci denote its amortized cost\nwith respect to ˆ, numi denote the number of items stored in the table after the ith\noperation, sizei denote the total size of the table after the ith operation, ˛i denote\nthe load factor of the table after the ith operation, and ˆi denote the potential after\nthe ith operation. Initially, num0 D 0, size0 D 0, ˛0 D 1, and ˆ0 D 0.\nWe start with the case in which the ith operation is TABLE-INSERT. The analy-\nsis is identical to that for table expansion in Section 17.4.1 if ˛i\u00031 \u0006 1=2. Whether\nthe table expands or not, the amortized cost yci of the operation is at most 3.\nIf ˛i\u00031 < 1=2, the table cannot expand as a result of the operation, since the ta-\nble expands only when ˛i\u00031 D 1. If ˛i < 1=2 as well, then the amortized cost of\nthe ith operation is\nyci\nD\nci C ˆi \u0005 ˆi\u00031\nD\n1 C .sizei=2 \u0005 numi/ \u0005 .sizei\u00031=2 \u0005 numi\u00031/\nD\n1 C .sizei=2 \u0005 numi/ \u0005 .sizei=2 \u0005 .numi \u0005 1//\nD\n0 :\nIf ˛i\u00031 < 1=2 but ˛i \u0006 1=2, then\nyci\nD\nci C ˆi \u0005 ˆi\u00031\nD\n1 C .2 \u0003 numi \u0005 sizei/ \u0005 .sizei\u00031=2 \u0005 numi\u00031/\nD\n1 C .2.numi\u00031 C 1/ \u0005 sizei\u00031/ \u0005 .sizei\u00031=2 \u0005 numi\u00031/\nD\n3 \u0003 numi\u00031 \u0005 3\n2sizei\u00031 C 3\nD\n3˛i\u00031sizei\u00031 \u0005 3\n2sizei\u00031 C 3\n<\n3\n2sizei\u00031 \u0005 3\n2sizei\u00031 C 3\nD\n3 :\nThus, the amortized cost of a TABLE-INSERT operation is at most 3.\nWe now turn to the case in which the ith operation is TABLE-DELETE. In this\ncase, numi D numi\u00031 \u0005 1. If ˛i\u00031 < 1=2, then we must consider whether the\noperation causes the table to contract. If it does not, then sizei D sizei\u00031 and the\n\nexpansion, and so forth. The cost of each expansion and contraction is ‚.n/, and\nthere are ‚.n/ of them. Thus, the total cost of the n operations is ‚.n2/, making\nthe amortized cost of an operation ‚.n/.\nThe downside of this strategy is obvious: after expanding the table, we do not\ndelete enough items to pay for a contraction. Likewise, after contracting the table,\nwe do not insert enough items to pay for an expansion.\nWe can improve upon this strategy by allowing the load factor of the table to\ndrop below 1=2. Speciﬁcally, we continue to double the table size upon inserting\nan item into a full table, but we halve the table size when deleting an item causes\nthe table to become less than 1=4 full, rather than 1=2 full as before. The load\nfactor of the table is therefore bounded below by the constant 1=4.\nIntuitively, we would consider a load factor of 1=2 to be ideal, and the table’s\npotential would then be 0. As the load factor deviates from 1=2, the potential\nincreases so that by the time we expand or contract the table, the table has garnered\nsufﬁcient potential to pay for copying all the items into the newly allocated table.\nThus, we will need a potential function that has grown to T:num by the time that\nthe load factor has either increased to 1 or decreased to 1=4. After either expanding\nor contracting the table, the load factor goes back to 1=2 and the table’s potential\nreduces back to 0.\nWe omit the code for TABLE-DELETE, since it is analogous to TABLE-INSERT.\nFor our analysis, we shall assume that whenever the number of items in the table\ndrops to 0, we free the storage for the table. That is, if T:num D 0, then T:size D 0.\nWe can now use the potential method to analyze the cost of a sequence of n\nTABLE-INSERT and TABLE-DELETE operations. We start by deﬁning a poten-\ntial function ˆ that is 0 immediately after an expansion or contraction and builds\nas the load factor increases to 1 or decreases to 1=4. Let us denote the load fac-\n17.4\nDynamic tables\n469\nnumi\n\nimmediately after an expansion. Then the table holds m=2 items, and it contains\n466\nChapter 17\nAmortized Analysis\nno credit. We charge 3 dollars for each insertion. The elementary insertion that\noccurs immediately costs 1 dollar. We place another dollar as credit on the item\ninserted. We place the third dollar as credit on one of the m=2 items already in the\ntable. The table will not ﬁll again until we have inserted another m=2 \u0005 1 items,\nand thus, by the time the table contains m items and is full, we will have placed a\ndollar on each item to pay to reinsert it during the expansion.\nWe can use the potential method to analyze a sequence of n TABLE-INSERT\noperations, and we shall use it in Section 17.4.2 to design a TABLE-DELETE op-\neration that has an O.1/ amortized cost as well. We start by deﬁning a potential\nfunction ˆ that is 0 immediately after an expansion but builds to the table size by\nthe time the table is full, so that we can pay for the next expansion by the potential.\nThe function\nˆ.T / D 2 \u0003 T:num \u0005 T:size\n(17.5)\nis one possibility. Immediately after an expansion, we have T:num D T:size=2,\nand thus ˆ.T / D 0, as desired.\nImmediately before an expansion, we have\nT:num D T:size, and thus ˆ.T / D T:num, as desired. The initial value of the\npotential is 0, and since the table is always at least half full, T:num \u0006 T:size=2,\nwhich implies that ˆ.T / is always nonnegative. Thus, the sum of the amortized\ncosts of n TABLE-INSERT operations gives an upper bound on the sum of the actual\ncosts.\nTo analyze the amortized cost of the ith TABLE-INSERT operation, we let numi\ndenote the number of items stored in the table after the ith operation, sizei denote\nthe total size of the table after the ith operation, and ˆi denote the potential after\nthe ith operation. Initially, we have num0 D 0, size0 D 0, and ˆ0 D 0.\nIf the ith TABLE-INSERT operation does not trigger an expansion, then we have\nsizei D sizei\u00031 and the amortized cost of the operation is\nyci\nD\nci C ˆi \u0005 ˆi\u00031\n\nthat ˆ.Di/ \u0006 ˆ.D0/, the total amortized cost of n operations is an upper bound\non the total actual cost. The worst-case cost of n operations is therefore O.n/.\nIncrementing a binary counter\nAs another example of the potential method, we again look at incrementing a binary\ncounter. This time, we deﬁne the potential of the counter after the ith INCREMENT\noperation to be bi, the number of 1s in the counter after the ith operation.\nLet us compute the amortized cost of an INCREMENT operation. Suppose that\nthe ith INCREMENT operation resets ti bits. The actual cost of the operation is\ntherefore at most ti C 1, since in addition to resetting ti bits, it sets at most one\nbit to 1. If bi D 0, then the ith operation resets all k bits, and so bi\u00031 D ti D k.\nIf bi > 0, then bi D bi\u00031 \u0005 ti C 1. In either case, bi \u0002 bi\u00031 \u0005 ti C 1, and the\npotential difference is\nˆ.Di/ \u0005 ˆ.Di\u00031/\n\u0002\n.bi\u00031 \u0005 ti C 1/ \u0005 bi\u00031\nD\n1 \u0005 ti :\nThe amortized cost is therefore\nyci\nD\nci C ˆ.Di/ \u0005 ˆ.Di\u00031/\n\u0002\n.ti C 1/ C .1 \u0005 ti/\nD\n2 :\nIf the counter starts at zero, then ˆ.D0/ D 0. Since ˆ.Di/ \u0006 0 for all i, the total\namortized cost of a sequence of n INCREMENT operations is an upper bound on the\ntotal actual cost, and so the worst-case cost of n INCREMENT operations is O.n/.\nThe potential method gives us an easy way to analyze the counter even when\nit does not start at zero. The counter starts with b0 1s, and after n INCREMENT\n462\nChapter 17\nAmortized Analysis\noperations it has bn 1s, where 0 \u0002 b0; bn \u0002 k. (Recall that k is the number of bits\nin the counter.) We can rewrite equation (17.3) as\nn\nX\niD1\nci D\nn\nX\niD1\nyci \u0005 ˆ.Dn/ C ˆ.D0/ :\n(17.4)\nWe have yci \u0002 2 for all 1 \u0002 i \u0002 n. Since ˆ.D0/ D b0 and ˆ.Dn/ D bn, the total\nactual cost of n INCREMENT operations is\nn\nX\niD1\nci\n\u0002\nn\nX\niD1\n2 \u0005 bn C b0\nD\n2n \u0005 bn C b0 :\nNote in particular that since b0 \u0002 k, as long as k D O.n/, the total actual cost\nis O.n/. In other words, if we execute at least n D \u0004.k/ INCREMENT operations,\n\n問題：Using the dynamic table expansion problem as an example, explain how the \"potential method\" of amortized analysis works. Define the potential function and show how it pays for expensive operations.\n\n回答：",
        "llm_final_answer": "好的，讓我來解釋一下動態表擴展問題，並說明「勢能方法」的運作方式。\n\n**勢能方法概述**\n\n勢能方法是一種攤銷分析技術，用於分析一系列操作的成本。它涉及定義一個「勢能函數」，該函數與數據結構的狀態相關聯。勢能函數可以被視為數據結構中「儲存」的能量。\n\n**動態表擴展問題的例子**\n\n1.  **問題背景**：考慮一個動態表，它會隨著項目的插入而增長。當表滿時，我們需要擴展它。擴展表需要分配一個新的、更大的表，並將所有現有項目複製到新表中，這是一個昂貴的操作。\n\n2.  **潛能函數的定義**：\n    *   `ˆ(T) = 2 * T:num - T:size`\n        *   `T:num`: 表中存儲的項目數量。\n        *   `T:size`: 表的總大小。\n    *   在擴展後，`T:num = T:size / 2`，因此 `ˆ(T) = 0`。\n    *   在擴展前，`T:num = T:size`，因此 `ˆ(T) = T:num`。\n    *   初始狀態下，`ˆ(T) = 0`。\n    *   由於表格至少有一半被佔用，`T:num ≥ T:size/2`，這意味著 `ˆ(T)` 始終是非負的。\n\n3.  **勢能如何支付昂貴的操作**：\n    *   **插入操作（未觸發擴展）**：如果第 i 次插入操作沒有觸發擴展，則 `size_i = size_{i-1}`。因此，攤銷成本 `yci = ci + ˆi - ˆ_{i-1}`。\n    *   **插入操作（觸發擴展）**：如果第 i 次插入操作觸發了擴展，實際成本 `ci` 包括將所有 `num_i - 1` 個項目複製到新表。勢能函數確保在擴展時有足夠的勢能來支付擴展的成本。在擴展之前，勢能是 `T:num`。擴展後，勢能重置為 0。\n\n4.  **攤銷成本的計算**：攤銷成本 `yci` 被定義為實際成本加上勢能的變化。當插入操作後，勢能增加時，攤銷成本會降低，反之亦然。\n\n    例如：如果 `ci = 1` （正常插入的成本），`ˆi - ˆ_{i-1} = 0`，那麼 `yci = 1 + 0 = 1`。如果擴展時 `ci` 很大，但 `ˆi - ˆ_{i-1}` 是負的，則攤銷成本會降低，因為勢能「支付」了擴展的成本。\n\n**關鍵思想**：\n\n*   勢能函數捕捉了數據結構的「狀態」，並允許我們「預付」未來昂貴操作的成本。\n*   通過精心設計勢能函數，我們可以證明摊销成本的上限，這說明了即使某些操作的實際成本很高，摊销成本仍然保持很低。\n\n總之，勢能方法通過定義和跟踪勢能函數，將昂貴操作的成本「攤銷」到一系列操作中，從而為我們提供了一種分析算法的有效方法。",
        "error_message": null,
        "question_id": "EN_AMORTIZED_POTENTIAL_METHOD_001"
    },
    {
        "original_question": "比較「基於比較的排序演算法 (comparison sort)」與「非基於比較的排序演算法（如計數排序 Counting Sort）」之間的根本區別是什麼？並說明計數排序的時間複雜度及其限制。",
        "decomposed_queries_text": "1.  什麼是基於比較的排序演算法 (comparison sort)？\n2.  什麼是非基於比較的排序演算法？\n3.  基於比較的排序演算法與非基於比較的排序演算法之間的主要區別是什麼？\n4.  計數排序 (Counting Sort) 是如何運作的？\n5.  計數排序的時間複雜度是多少？\n6.  計數排序有哪些限制？",
        "hypothetical_document": "基於比較的排序演算法，例如合併排序或快速排序，其複雜度下限為 Ω(n log n)，因為它們需要透過元素間的成對比較來確定順序。相反，非基於比較的排序演算法，如計數排序，利用鍵值的分布特性，在已知鍵值範圍的情況下，以 O(n+k) 的時間複雜度完成排序，其中 k 為鍵值範圍的大小。 然而，計數排序的主要限制在於其高額的空間複雜度，特別是當 k 值很大時，不適合處理鍵值分布範圍廣泛或負數的數據。",
        "query_for_retrieval": "基於比較的排序演算法，例如合併排序或快速排序，其複雜度下限為 Ω(n log n)，因為它們需要透過元素間的成對比較來確定順序。相反，非基於比較的排序演算法，如計數排序，利用鍵值的分布特性，在已知鍵值範圍的情況下，以 O(n+k) 的時間複雜度完成排序，其中 k 為鍵值範圍的大小。 然而，計數排序的主要限制在於其高額的空間複雜度，特別是當 k 值很大時，不適合處理鍵值分布範圍廣泛或負數的數據。",
        "retrieved_contexts_from_server": [
            {
                "text": "comparisons in the worst case to sort n elements. Thus, merge sort and heapsort\nare asymptotically optimal, and no comparison sort exists that is faster by more\nthan a constant factor.\nSections 8.2, 8.3, and 8.4 examine three sorting algorithms—counting sort, radix\nsort, and bucket sort—that run in linear time. Of course, these algorithms use\noperations other than comparisons to determine the sorted order. Consequently,\nthe \u0004.n lg n/ lower bound does not apply to them.\n8.1\nLower bounds for sorting\nIn a comparison sort, we use only comparisons between elements to gain order\ninformation about an input sequence ha1; a2; : : : ; ani. That is, given two elements\nai and aj, we perform one of the tests ai < aj, ai \u0002 aj, ai D aj, ai \u0006 aj, or\nai > aj to determine their relative order. We may not inspect the values of the\nelements or gain order information about them in any other way.\nIn this section, we assume without loss of generality that all the input elements\nare distinct. Given this assumption, comparisons of the form ai D aj are useless,\nso we can assume that no comparisons of this form are made. We also note that\nthe comparisons ai \u0002 aj, ai \u0006 aj, ai > aj, and ai < aj are all equivalent in that\n192\nChapter 8\nSorting in Linear Time\n≤\n>\n≤\n>\n1:2\n2:3\n1:3\n〈1,2,3〉\n1:3\n〈2,1,3〉\n2:3\n〈1,3,2〉\n〈3,1,2〉\n〈3,2,1〉\n≤\n>\n≤\n>\n≤\n>\n〈2,3,1〉\nFigure 8.1\nThe decision tree for insertion sort operating on three elements. An internal node an-\nnotated by i:j indicates a comparison between ai and aj . A leaf annotated by the permutation\nh\u0006.1/; \u0006.2/; : : : ; \u0006.n/i indicates the ordering a\u0003.1/ \u0002 a\u0003.2/ \u0002 \u0003 \u0003 \u0003 \u0002 a\u0003.n/. The shaded path\nindicates the decisions made when sorting the input sequence ha1 D 6; a2 D 8; a3 D 5i; the\npermutation h3; 1; 2i at the leaf indicates that the sorted ordering is a3 D 5 \u0002 a1 D 6 \u0002 a2 D 8.\nThere are 3Š D 6 possible permutations of the input elements, and so the decision tree must have at\nleast 6 leaves.",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_4ebb9595-44f0-4930-99e4-05bf68f5e26a"
                }
            },
            {
                "text": "so the hidden constant factor in its running time is small. It is a popular algorithm\nfor sorting large input arrays.\nInsertion sort, merge sort, heapsort, and quicksort are all comparison sorts: they\ndetermine the sorted order of an input array by comparing elements. Chapter 8 be-\ngins by introducing the decision-tree model in order to study the performance limi-\ntations of comparison sorts. Using this model, we prove a lower bound of \u0004.n lg n/\non the worst-case running time of any comparison sort on n inputs, thus showing\nthat heapsort and merge sort are asymptotically optimal comparison sorts.\nChapter 8 then goes on to show that we can beat this lower bound of \u0004.n lg n/\nif we can gather information about the sorted order of the input by means other\nthan comparing elements. The counting sort algorithm, for example, assumes that\nthe input numbers are in the set f0; 1; : : : ; kg. By using array indexing as a tool\nfor determining relative order, counting sort can sort n numbers in ‚.k C n/ time.\nThus, when k D O.n/, counting sort runs in time that is linear in the size of the\ninput array. A related algorithm, radix sort, can be used to extend the range of\ncounting sort. If there are n integers to sort, each integer has d digits, and each\ndigit can take on up to k possible values, then radix sort can sort the numbers\nin ‚.d.n C k// time. When d is a constant and k is O.n/, radix sort runs in\nlinear time. A third algorithm, bucket sort, requires knowledge of the probabilistic\ndistribution of numbers in the input array. It can sort n real numbers uniformly\ndistributed in the half-open interval Œ0; 1/ in average-case O.n/ time.\nThe following table summarizes the running times of the sorting algorithms from\nChapters 2 and 6–8. As usual, n denotes the number of items to sort. For counting\nsort, the items to sort are integers in the set f0; 1; : : : ; kg. For radix sort, each item\nis a d-digit number, where each digit takes on k possible values. For bucket sort,",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_4400b4e3-f217-4fc4-9c37-677c7eecad4c"
                }
            },
            {
                "text": "array C to all zeros, the for loop of lines 4–5 inspects each input element. If the\nvalue of an input element is i, we increment CŒi\u0002. Thus, after line 5, CŒi\u0002 holds\nthe number of input elements equal to i for each integer i D 0; 1; : : : ; k. Lines 7–8\ndetermine for each i D 0; 1; : : : ; k how many input elements are less than or equal\nto i by keeping a running sum of the array C.\n196\nChapter 8\nSorting in Linear Time\nFinally, the for loop of lines 10–12 places each element AŒj \u0002 into its correct\nsorted position in the output array B. If all n elements are distinct, then when we\nﬁrst enter line 10, for each AŒj \u0002, the value CŒAŒj \u0002\u0002 is the correct ﬁnal position\nof AŒj \u0002 in the output array, since there are CŒAŒj \u0002\u0002 elements less than or equal\nto AŒj \u0002. Because the elements might not be distinct, we decrement CŒAŒj \u0002\u0002 each\ntime we place a value AŒj \u0002 into the B array. Decrementing CŒAŒj \u0002\u0002 causes the\nnext input element with a value equal to AŒj \u0002, if one exists, to go to the position\nimmediately before AŒj \u0002 in the output array.\nHow much time does counting sort require? The for loop of lines 2–3 takes\ntime ‚.k/, the for loop of lines 4–5 takes time ‚.n/, the for loop of lines 7–8 takes\ntime ‚.k/, and the for loop of lines 10–12 takes time ‚.n/. Thus, the overall time\nis ‚.k C n/. In practice, we usually use counting sort when we have k D O.n/, in\nwhich case the running time is ‚.n/.\nCounting sort beats the lower bound of \u0004.n lg n/ proved in Section 8.1 because\nit is not a comparison sort. In fact, no comparisons between input elements occur\nanywhere in the code. Instead, counting sort uses the actual values of the elements\nto index into an array. The \u0004.n lg n/ lower bound for sorting does not apply when\nwe depart from the comparison sort model.\nAn important property of counting sort is that it is stable: numbers with the same\nvalue appear in the output array in the same order as they do in the input array. That",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_72d3e1cd-07be-4a1d-8361-86cc1dbc59e2"
                }
            },
            {
                "text": "D\n\u0004.n lg n/\n(by equation (3.19)) .\nCorollary 8.2\nHeapsort and merge sort are asymptotically optimal comparison sorts.\nProof\nThe O.n lg n/ upper bounds on the running times for heapsort and merge\nsort match the \u0004.n lg n/ worst-case lower bound from Theorem 8.1.\nExercises\n8.1-1\nWhat is the smallest possible depth of a leaf in a decision tree for a comparison\nsort?\n194\nChapter 8\nSorting in Linear Time\n8.1-2\nObtain asymptotically tight bounds on lg.nŠ/ without using Stirling’s approxi-\nmation. Instead, evaluate the summation Pn\nkD1 lg k using techniques from Sec-\ntion A.2.\n8.1-3\nShow that there is no comparison sort whose running time is linear for at least half\nof the nŠ inputs of length n. What about a fraction of 1=n of the inputs of length n?\nWhat about a fraction 1=2n?\n8.1-4\nSuppose that you are given a sequence of n elements to sort. The input sequence\nconsists of n=k subsequences, each containing k elements. The elements in a given\nsubsequence are all smaller than the elements in the succeeding subsequence and\nlarger than the elements in the preceding subsequence. Thus, all that is needed to\nsort the whole sequence of length n is to sort the k elements in each of the n=k\nsubsequences. Show an \u0004.n lg k/ lower bound on the number of comparisons\nneeded to solve this variant of the sorting problem. (Hint: It is not rigorous to\nsimply combine the lower bounds for the individual subsequences.)\n8.2\nCounting sort\nCounting sort assumes that each of the n input elements is an integer in the range\n0 to k, for some integer k. When k D O.n/, the sort runs in ‚.n/ time.\nCounting sort determines, for each input element x, the number of elements less\nthan x. It uses this information to place element x directly into its position in the\noutput array. For example, if 17 elements are less than x, then x belongs in output\nposition 18. We must modify this scheme slightly to handle the situation in which\nseveral elements have the same value, since we do not want to put them all in the",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_98008b55-9781-4292-ad36-9f44949eb593"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\ncomparisons in the worst case to sort n elements. Thus, merge sort and heapsort\nare asymptotically optimal, and no comparison sort exists that is faster by more\nthan a constant factor.\nSections 8.2, 8.3, and 8.4 examine three sorting algorithms—counting sort, radix\nsort, and bucket sort—that run in linear time. Of course, these algorithms use\noperations other than comparisons to determine the sorted order. Consequently,\nthe \u0004.n lg n/ lower bound does not apply to them.\n8.1\nLower bounds for sorting\nIn a comparison sort, we use only comparisons between elements to gain order\ninformation about an input sequence ha1; a2; : : : ; ani. That is, given two elements\nai and aj, we perform one of the tests ai < aj, ai \u0002 aj, ai D aj, ai \u0006 aj, or\nai > aj to determine their relative order. We may not inspect the values of the\nelements or gain order information about them in any other way.\nIn this section, we assume without loss of generality that all the input elements\nare distinct. Given this assumption, comparisons of the form ai D aj are useless,\nso we can assume that no comparisons of this form are made. We also note that\nthe comparisons ai \u0002 aj, ai \u0006 aj, ai > aj, and ai < aj are all equivalent in that\n192\nChapter 8\nSorting in Linear Time\n≤\n>\n≤\n>\n1:2\n2:3\n1:3\n〈1,2,3〉\n1:3\n〈2,1,3〉\n2:3\n〈1,3,2〉\n〈3,1,2〉\n〈3,2,1〉\n≤\n>\n≤\n>\n≤\n>\n〈2,3,1〉\nFigure 8.1\nThe decision tree for insertion sort operating on three elements. An internal node an-\nnotated by i:j indicates a comparison between ai and aj . A leaf annotated by the permutation\nh\u0006.1/; \u0006.2/; : : : ; \u0006.n/i indicates the ordering a\u0003.1/ \u0002 a\u0003.2/ \u0002 \u0003 \u0003 \u0003 \u0002 a\u0003.n/. The shaded path\nindicates the decisions made when sorting the input sequence ha1 D 6; a2 D 8; a3 D 5i; the\npermutation h3; 1; 2i at the leaf indicates that the sorted ordering is a3 D 5 \u0002 a1 D 6 \u0002 a2 D 8.\nThere are 3Š D 6 possible permutations of the input elements, and so the decision tree must have at\nleast 6 leaves.\n\nso the hidden constant factor in its running time is small. It is a popular algorithm\nfor sorting large input arrays.\nInsertion sort, merge sort, heapsort, and quicksort are all comparison sorts: they\ndetermine the sorted order of an input array by comparing elements. Chapter 8 be-\ngins by introducing the decision-tree model in order to study the performance limi-\ntations of comparison sorts. Using this model, we prove a lower bound of \u0004.n lg n/\non the worst-case running time of any comparison sort on n inputs, thus showing\nthat heapsort and merge sort are asymptotically optimal comparison sorts.\nChapter 8 then goes on to show that we can beat this lower bound of \u0004.n lg n/\nif we can gather information about the sorted order of the input by means other\nthan comparing elements. The counting sort algorithm, for example, assumes that\nthe input numbers are in the set f0; 1; : : : ; kg. By using array indexing as a tool\nfor determining relative order, counting sort can sort n numbers in ‚.k C n/ time.\nThus, when k D O.n/, counting sort runs in time that is linear in the size of the\ninput array. A related algorithm, radix sort, can be used to extend the range of\ncounting sort. If there are n integers to sort, each integer has d digits, and each\ndigit can take on up to k possible values, then radix sort can sort the numbers\nin ‚.d.n C k// time. When d is a constant and k is O.n/, radix sort runs in\nlinear time. A third algorithm, bucket sort, requires knowledge of the probabilistic\ndistribution of numbers in the input array. It can sort n real numbers uniformly\ndistributed in the half-open interval Œ0; 1/ in average-case O.n/ time.\nThe following table summarizes the running times of the sorting algorithms from\nChapters 2 and 6–8. As usual, n denotes the number of items to sort. For counting\nsort, the items to sort are integers in the set f0; 1; : : : ; kg. For radix sort, each item\nis a d-digit number, where each digit takes on k possible values. For bucket sort,\n\narray C to all zeros, the for loop of lines 4–5 inspects each input element. If the\nvalue of an input element is i, we increment CŒi\u0002. Thus, after line 5, CŒi\u0002 holds\nthe number of input elements equal to i for each integer i D 0; 1; : : : ; k. Lines 7–8\ndetermine for each i D 0; 1; : : : ; k how many input elements are less than or equal\nto i by keeping a running sum of the array C.\n196\nChapter 8\nSorting in Linear Time\nFinally, the for loop of lines 10–12 places each element AŒj \u0002 into its correct\nsorted position in the output array B. If all n elements are distinct, then when we\nﬁrst enter line 10, for each AŒj \u0002, the value CŒAŒj \u0002\u0002 is the correct ﬁnal position\nof AŒj \u0002 in the output array, since there are CŒAŒj \u0002\u0002 elements less than or equal\nto AŒj \u0002. Because the elements might not be distinct, we decrement CŒAŒj \u0002\u0002 each\ntime we place a value AŒj \u0002 into the B array. Decrementing CŒAŒj \u0002\u0002 causes the\nnext input element with a value equal to AŒj \u0002, if one exists, to go to the position\nimmediately before AŒj \u0002 in the output array.\nHow much time does counting sort require? The for loop of lines 2–3 takes\ntime ‚.k/, the for loop of lines 4–5 takes time ‚.n/, the for loop of lines 7–8 takes\ntime ‚.k/, and the for loop of lines 10–12 takes time ‚.n/. Thus, the overall time\nis ‚.k C n/. In practice, we usually use counting sort when we have k D O.n/, in\nwhich case the running time is ‚.n/.\nCounting sort beats the lower bound of \u0004.n lg n/ proved in Section 8.1 because\nit is not a comparison sort. In fact, no comparisons between input elements occur\nanywhere in the code. Instead, counting sort uses the actual values of the elements\nto index into an array. The \u0004.n lg n/ lower bound for sorting does not apply when\nwe depart from the comparison sort model.\nAn important property of counting sort is that it is stable: numbers with the same\nvalue appear in the output array in the same order as they do in the input array. That\n\nD\n\u0004.n lg n/\n(by equation (3.19)) .\nCorollary 8.2\nHeapsort and merge sort are asymptotically optimal comparison sorts.\nProof\nThe O.n lg n/ upper bounds on the running times for heapsort and merge\nsort match the \u0004.n lg n/ worst-case lower bound from Theorem 8.1.\nExercises\n8.1-1\nWhat is the smallest possible depth of a leaf in a decision tree for a comparison\nsort?\n194\nChapter 8\nSorting in Linear Time\n8.1-2\nObtain asymptotically tight bounds on lg.nŠ/ without using Stirling’s approxi-\nmation. Instead, evaluate the summation Pn\nkD1 lg k using techniques from Sec-\ntion A.2.\n8.1-3\nShow that there is no comparison sort whose running time is linear for at least half\nof the nŠ inputs of length n. What about a fraction of 1=n of the inputs of length n?\nWhat about a fraction 1=2n?\n8.1-4\nSuppose that you are given a sequence of n elements to sort. The input sequence\nconsists of n=k subsequences, each containing k elements. The elements in a given\nsubsequence are all smaller than the elements in the succeeding subsequence and\nlarger than the elements in the preceding subsequence. Thus, all that is needed to\nsort the whole sequence of length n is to sort the k elements in each of the n=k\nsubsequences. Show an \u0004.n lg k/ lower bound on the number of comparisons\nneeded to solve this variant of the sorting problem. (Hint: It is not rigorous to\nsimply combine the lower bounds for the individual subsequences.)\n8.2\nCounting sort\nCounting sort assumes that each of the n input elements is an integer in the range\n0 to k, for some integer k. When k D O.n/, the sort runs in ‚.n/ time.\nCounting sort determines, for each input element x, the number of elements less\nthan x. It uses this information to place element x directly into its position in the\noutput array. For example, if 17 elements are less than x, then x belongs in output\nposition 18. We must modify this scheme slightly to handle the situation in which\nseveral elements have the same value, since we do not want to put them all in the\n\n問題：比較「基於比較的排序演算法 (comparison sort)」與「非基於比較的排序演算法（如計數排序 Counting Sort）」之間的根本區別是什麼？並說明計數排序的時間複雜度及其限制。\n\n回答：",
        "llm_final_answer": "根據提供的資料，以下是基於比較的排序演算法與非基於比較的排序演算法（如計數排序）之間的根本區別，以及計數排序的時間複雜度和限制：\n\n**1. 根本區別：**\n\n*   **基於比較的排序演算法 (Comparison Sort):** 這類演算法**僅**通過比較輸入數組中的元素來確定它們的相對順序。例如，它會比較 `ai < aj`、`ai > aj` 等。歸併排序、堆排序和快速排序都屬於此類。由於它們依賴於比較，它們的**最壞情況下時間複雜度下限為 \u0004.n lg n/。**\n\n*   **非基於比較的排序演算法:** 這類演算法使用**非比較**的其他方法來確定元素的順序。計數排序是其中之一，它使用元素的**實際值**來索引數組，從而確定相對順序，而不是比較元素。\n\n**2. 計數排序的時間複雜度：**\n\n*   計數排序的時間複雜度是 **‚.k + n/**，其中 `n` 是輸入元素的數量，`k` 是輸入元素的最大值。\n*   當 `k = O(n)` 時，計數排序的時間複雜度簡化為 **‚.n/**，這表示它以線性時間運行，比基於比較的排序演算法快。\n\n**3. 計數排序的限制：**\n\n*   計數排序的**主要限制是它僅適用於輸入元素是 0 到 k 之間整數的情況**。\n*   如果 k 的值非常大，計數排序所需的額外空間量（用於存儲計數的數組）會很大，這可能會使其不切實際。\n\n總而言之，計數排序通過避免比較，利用輸入元素的特定屬性（整數且在一定範圍內）來實現線性時間的排序。 然而，它有其應用範圍的限制，並且不適用於所有情況。",
        "error_message": null,
        "question_id": "ZH_SORT_COMPARISON_COUNTING_001"
    },
    {
        "original_question": "Explain the concept of a \"polynomial-time reduction\" (≤_P). Why is this concept so central to the theory of NP-completeness?",
        "decomposed_queries_text": "1.  What is a polynomial-time reduction (≤_P)? Define the concept.\n2.  What are the properties and characteristics of polynomial-time reductions?\n3.  How is polynomial-time reduction used to transform one problem into another?\n4.  What is NP-completeness? Define the concept.\n5.  How does polynomial-time reduction relate to NP-completeness? Explain the connection.\n6.  Why is polynomial-time reduction a crucial concept in the theory of NP-completeness? Discuss its significance.",
        "hypothetical_document": "A polynomial-time reduction (≤_P), denoted as a reduction from problem A to problem B, implies the existence of a polynomial-time algorithm that transforms any instance of A into an equivalent instance of B. This transformation allows us to leverage the complexity analysis of B to infer the complexity of A. Consequently, polynomial-time reductions are central to NP-completeness, as they are the crucial mechanism for demonstrating that a problem is NP-hard: if a known NP-complete problem reduces to another in polynomial time, the latter is also NP-hard.",
        "query_for_retrieval": "A polynomial-time reduction (≤_P), denoted as a reduction from problem A to problem B, implies the existence of a polynomial-time algorithm that transforms any instance of A into an equivalent instance of B. This transformation allows us to leverage the complexity analysis of B to infer the complexity of A. Consequently, polynomial-time reductions are central to NP-completeness, as they are the crucial mechanism for demonstrating that a problem is NP-hard: if a known NP-complete problem reduces to another in polynomial time, the latter is also NP-hard.",
        "retrieved_contexts_from_server": [
            {
                "text": "which we already know that no polynomial-time algorithm can exist. (Let us not\nconcern ourselves for now with how to ﬁnd such a problem A.) Suppose further\nthat we have a polynomial-time reduction transforming instances of A to instances\nof B. Now we can use a simple proof by contradiction to show that no polynomial-\ntime algorithm can exist for B. Suppose otherwise; i.e., suppose that B has a\npolynomial-time algorithm. Then, using the method shown in Figure 34.1, we\nwould have a way to solve problem A in polynomial time, which contradicts our\nassumption that there is no polynomial-time algorithm for A.\nFor NP-completeness, we cannot assume that there is absolutely no polynomial-\ntime algorithm for problem A. The proof methodology is similar, however, in that\nwe prove that problem B is NP-complete on the assumption that problem A is also\nNP-complete.\n34.1\nPolynomial time\n1053\nA ﬁrst NP-complete problem\nBecause the technique of reduction relies on having a problem already known to\nbe NP-complete in order to prove a different problem NP-complete, we need a\n“ﬁrst” NP-complete problem. The problem we shall use is the circuit-satisﬁability\nproblem, in which we are given a boolean combinational circuit composed of AND,\nOR, and NOT gates, and we wish to know whether there exists some set of boolean\ninputs to this circuit that causes its output to be 1. We shall prove that this ﬁrst\nproblem is NP-complete in Section 34.3.\nChapter outline\nThis chapter studies the aspects of NP-completeness that bear most directly on the\nanalysis of algorithms. In Section 34.1, we formalize our notion of “problem” and\ndeﬁne the complexity class P of polynomial-time solvable decision problems. We\nalso see how these notions ﬁt into the framework of formal-language theory. Sec-\ntion 34.2 deﬁnes the class NP of decision problems whose solutions are veriﬁable\nin polynomial time. It also formally poses the P ¤ NP question.\nSection 34.3 shows we can relate problems via polynomial-time “reductions.”",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_481d9579-3557-4388-b503-ce72778de270"
                }
            },
            {
                "text": "tion 34.2 deﬁnes the class NP of decision problems whose solutions are veriﬁable\nin polynomial time. It also formally poses the P ¤ NP question.\nSection 34.3 shows we can relate problems via polynomial-time “reductions.”\nIt deﬁnes NP-completeness and sketches a proof that one problem, called “circuit\nsatisﬁability,” is NP-complete. Having found one NP-complete problem, we show\nin Section 34.4 how to prove other problems to be NP-complete much more simply\nby the methodology of reductions. We illustrate this methodology by showing that\ntwo formula-satisﬁability problems are NP-complete. With additional reductions,\nwe show in Section 34.5 a variety of other problems to be NP-complete.\n34.1\nPolynomial time\nWe begin our study of NP-completeness by formalizing our notion of polynomial-\ntime solvable problems. We generally regard these problems as tractable, but for\nphilosophical, not mathematical, reasons.\nWe can offer three supporting argu-\nments.\nFirst, although we may reasonably regard a problem that requires time ‚.n100/\nto be intractable, very few practical problems require time on the order of such a\nhigh-degree polynomial. The polynomial-time computable problems encountered\nin practice typically require much less time. Experience has shown that once the\nﬁrst polynomial-time algorithm for a problem has been discovered, more efﬁcient\nalgorithms often follow. Even if the current best algorithm for a problem has a\nrunning time of ‚.n100/, an algorithm with a much better running time will likely\nsoon be discovered.\n1054\nChapter 34\nNP-Completeness\nSecond, for many reasonable models of computation, a problem that can be\nsolved in polynomial time in one model can be solved in polynomial time in an-\nother. For example, the class of problems solvable in polynomial time by the serial\nrandom-access machine used throughout most of this book is the same as the class\nof problems solvable in polynomial time on abstract Turing machines.1 It is also",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_b903e6aa-a7bd-423a-b85b-06a18b5f2956"
                }
            },
            {
                "text": "\u0002\nThe answers are the same. That is, the answer for ˛ is “yes” if and only if the\nanswer for ˇ is also “yes.”\n1052\nChapter 34\nNP-Completeness\npolynomial-time\nreduction algorithm\ninstance β\npolynomial-time\nalgorithm to decide B\nyes\nyes\npolynomial-time algorithm to decide A\nno\nno\nof B\ninstance α\nof A\nFigure 34.1\nHow to use a polynomial-time reduction algorithm to solve a decision problem A in\npolynomial time, given a polynomial-time decision algorithm for another problem B. In polynomial\ntime, we transform an instance ˛ of A into an instance ˇ of B, we solve B in polynomial time, and\nwe use the answer for ˇ as the answer for ˛.\nWe call such a procedure a polynomial-time reduction algorithm and, as Fig-\nure 34.1 shows, it provides us a way to solve problem A in polynomial time:\n1. Given an instance ˛ of problem A, use a polynomial-time reduction algorithm\nto transform it to an instance ˇ of problem B.\n2. Run the polynomial-time decision algorithm for B on the instance ˇ.\n3. Use the answer for ˇ as the answer for ˛.\nAs long as each of these steps takes polynomial time, all three together do also, and\nso we have a way to decide on ˛ in polynomial time. In other words, by “reducing”\nsolving problem A to solving problem B, we use the “easiness” of B to prove the\n“easiness” of A.\nRecalling that NP-completeness is about showing how hard a problem is rather\nthan how easy it is, we use polynomial-time reductions in the opposite way to show\nthat a problem is NP-complete. Let us take the idea a step further, and show how we\ncould use polynomial-time reductions to show that no polynomial-time algorithm\ncan exist for a particular problem B. Suppose we have a decision problem A for\nwhich we already know that no polynomial-time algorithm can exist. (Let us not\nconcern ourselves for now with how to ﬁnd such a problem A.) Suppose further\nthat we have a polynomial-time reduction transforming instances of A to instances",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_c6d3d8c2-bc7d-4f70-98ce-a55a4aa5346a"
                }
            },
            {
                "text": "via a reduction function f . For any input x 2 f0; 1g\u0004, the question of whether x 2 L1 has the same\nanswer as the question of whether f .x/ 2 L2.\nthen f .x/ 2 L2. Moreover, if x 62 L1, then f .x/ 62 L2. Thus, the reduction func-\ntion maps any instance x of the decision problem represented by the language L1\nto an instance f .x/ of the problem represented by L2. Providing an answer to\nwhether f .x/ 2 L2 directly provides the answer to whether x 2 L1.\nPolynomial-time reductions give us a powerful tool for proving that various lan-\nguages belong to P.\nLemma 34.3\nIf L1; L2 \u0007 f0; 1g\u0004 are languages such that L1 \u0002P L2, then L2 2 P implies\nL1 2 P.\nProof\nLet A2 be a polynomial-time algorithm that decides L2, and let F be a\npolynomial-time reduction algorithm that computes the reduction function f . We\nshall construct a polynomial-time algorithm A1 that decides L1.\nFigure 34.5 illustrates how we construct A1. For a given input x 2 f0; 1g\u0004,\nalgorithm A1 uses F to transform x into f .x/, and then it uses A2 to test whether\nf .x/ 2 L2. Algorithm A1 takes the output from algorithm A2 and produces that\nanswer as its own output.\nThe correctness of A1 follows from condition (34.1). The algorithm runs in poly-\nnomial time, since both F and A2 run in polynomial time (see Exercise 34.1-5).\nNP-completeness\nPolynomial-time reductions provide a formal means for showing that one prob-\nlem is at least as hard as another, to within a polynomial-time factor. That is, if\nL1 \u0002P L2, then L1 is not more than a polynomial factor harder than L2, which is\n34.3\nNP-completeness and reducibility\n1069\nx\nF\nf .x/\nA1\nA2\nyes, f .x/ 2 L2\nno, f .x/ 62 L2\nyes, x 2 L1\nno, x 62 L1\nFigure 34.5\nThe proof of Lemma 34.3. The algorithm F is a reduction algorithm that computes the\nreduction function f from L1 to L2 in polynomial time, and A2 is a polynomial-time algorithm that\ndecides L2. Algorithm A1 decides whether x 2 L1 by using F to transform any input x into f .x/\nand then using A2 to decide whether f .x/ 2 L2.",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_d5f088d1-728c-4771-94d2-02f86aa5e5d4"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nwhich we already know that no polynomial-time algorithm can exist. (Let us not\nconcern ourselves for now with how to ﬁnd such a problem A.) Suppose further\nthat we have a polynomial-time reduction transforming instances of A to instances\nof B. Now we can use a simple proof by contradiction to show that no polynomial-\ntime algorithm can exist for B. Suppose otherwise; i.e., suppose that B has a\npolynomial-time algorithm. Then, using the method shown in Figure 34.1, we\nwould have a way to solve problem A in polynomial time, which contradicts our\nassumption that there is no polynomial-time algorithm for A.\nFor NP-completeness, we cannot assume that there is absolutely no polynomial-\ntime algorithm for problem A. The proof methodology is similar, however, in that\nwe prove that problem B is NP-complete on the assumption that problem A is also\nNP-complete.\n34.1\nPolynomial time\n1053\nA ﬁrst NP-complete problem\nBecause the technique of reduction relies on having a problem already known to\nbe NP-complete in order to prove a different problem NP-complete, we need a\n“ﬁrst” NP-complete problem. The problem we shall use is the circuit-satisﬁability\nproblem, in which we are given a boolean combinational circuit composed of AND,\nOR, and NOT gates, and we wish to know whether there exists some set of boolean\ninputs to this circuit that causes its output to be 1. We shall prove that this ﬁrst\nproblem is NP-complete in Section 34.3.\nChapter outline\nThis chapter studies the aspects of NP-completeness that bear most directly on the\nanalysis of algorithms. In Section 34.1, we formalize our notion of “problem” and\ndeﬁne the complexity class P of polynomial-time solvable decision problems. We\nalso see how these notions ﬁt into the framework of formal-language theory. Sec-\ntion 34.2 deﬁnes the class NP of decision problems whose solutions are veriﬁable\nin polynomial time. It also formally poses the P ¤ NP question.\nSection 34.3 shows we can relate problems via polynomial-time “reductions.”\n\ntion 34.2 deﬁnes the class NP of decision problems whose solutions are veriﬁable\nin polynomial time. It also formally poses the P ¤ NP question.\nSection 34.3 shows we can relate problems via polynomial-time “reductions.”\nIt deﬁnes NP-completeness and sketches a proof that one problem, called “circuit\nsatisﬁability,” is NP-complete. Having found one NP-complete problem, we show\nin Section 34.4 how to prove other problems to be NP-complete much more simply\nby the methodology of reductions. We illustrate this methodology by showing that\ntwo formula-satisﬁability problems are NP-complete. With additional reductions,\nwe show in Section 34.5 a variety of other problems to be NP-complete.\n34.1\nPolynomial time\nWe begin our study of NP-completeness by formalizing our notion of polynomial-\ntime solvable problems. We generally regard these problems as tractable, but for\nphilosophical, not mathematical, reasons.\nWe can offer three supporting argu-\nments.\nFirst, although we may reasonably regard a problem that requires time ‚.n100/\nto be intractable, very few practical problems require time on the order of such a\nhigh-degree polynomial. The polynomial-time computable problems encountered\nin practice typically require much less time. Experience has shown that once the\nﬁrst polynomial-time algorithm for a problem has been discovered, more efﬁcient\nalgorithms often follow. Even if the current best algorithm for a problem has a\nrunning time of ‚.n100/, an algorithm with a much better running time will likely\nsoon be discovered.\n1054\nChapter 34\nNP-Completeness\nSecond, for many reasonable models of computation, a problem that can be\nsolved in polynomial time in one model can be solved in polynomial time in an-\nother. For example, the class of problems solvable in polynomial time by the serial\nrandom-access machine used throughout most of this book is the same as the class\nof problems solvable in polynomial time on abstract Turing machines.1 It is also\n\n\u0002\nThe answers are the same. That is, the answer for ˛ is “yes” if and only if the\nanswer for ˇ is also “yes.”\n1052\nChapter 34\nNP-Completeness\npolynomial-time\nreduction algorithm\ninstance β\npolynomial-time\nalgorithm to decide B\nyes\nyes\npolynomial-time algorithm to decide A\nno\nno\nof B\ninstance α\nof A\nFigure 34.1\nHow to use a polynomial-time reduction algorithm to solve a decision problem A in\npolynomial time, given a polynomial-time decision algorithm for another problem B. In polynomial\ntime, we transform an instance ˛ of A into an instance ˇ of B, we solve B in polynomial time, and\nwe use the answer for ˇ as the answer for ˛.\nWe call such a procedure a polynomial-time reduction algorithm and, as Fig-\nure 34.1 shows, it provides us a way to solve problem A in polynomial time:\n1. Given an instance ˛ of problem A, use a polynomial-time reduction algorithm\nto transform it to an instance ˇ of problem B.\n2. Run the polynomial-time decision algorithm for B on the instance ˇ.\n3. Use the answer for ˇ as the answer for ˛.\nAs long as each of these steps takes polynomial time, all three together do also, and\nso we have a way to decide on ˛ in polynomial time. In other words, by “reducing”\nsolving problem A to solving problem B, we use the “easiness” of B to prove the\n“easiness” of A.\nRecalling that NP-completeness is about showing how hard a problem is rather\nthan how easy it is, we use polynomial-time reductions in the opposite way to show\nthat a problem is NP-complete. Let us take the idea a step further, and show how we\ncould use polynomial-time reductions to show that no polynomial-time algorithm\ncan exist for a particular problem B. Suppose we have a decision problem A for\nwhich we already know that no polynomial-time algorithm can exist. (Let us not\nconcern ourselves for now with how to ﬁnd such a problem A.) Suppose further\nthat we have a polynomial-time reduction transforming instances of A to instances\n\nvia a reduction function f . For any input x 2 f0; 1g\u0004, the question of whether x 2 L1 has the same\nanswer as the question of whether f .x/ 2 L2.\nthen f .x/ 2 L2. Moreover, if x 62 L1, then f .x/ 62 L2. Thus, the reduction func-\ntion maps any instance x of the decision problem represented by the language L1\nto an instance f .x/ of the problem represented by L2. Providing an answer to\nwhether f .x/ 2 L2 directly provides the answer to whether x 2 L1.\nPolynomial-time reductions give us a powerful tool for proving that various lan-\nguages belong to P.\nLemma 34.3\nIf L1; L2 \u0007 f0; 1g\u0004 are languages such that L1 \u0002P L2, then L2 2 P implies\nL1 2 P.\nProof\nLet A2 be a polynomial-time algorithm that decides L2, and let F be a\npolynomial-time reduction algorithm that computes the reduction function f . We\nshall construct a polynomial-time algorithm A1 that decides L1.\nFigure 34.5 illustrates how we construct A1. For a given input x 2 f0; 1g\u0004,\nalgorithm A1 uses F to transform x into f .x/, and then it uses A2 to test whether\nf .x/ 2 L2. Algorithm A1 takes the output from algorithm A2 and produces that\nanswer as its own output.\nThe correctness of A1 follows from condition (34.1). The algorithm runs in poly-\nnomial time, since both F and A2 run in polynomial time (see Exercise 34.1-5).\nNP-completeness\nPolynomial-time reductions provide a formal means for showing that one prob-\nlem is at least as hard as another, to within a polynomial-time factor. That is, if\nL1 \u0002P L2, then L1 is not more than a polynomial factor harder than L2, which is\n34.3\nNP-completeness and reducibility\n1069\nx\nF\nf .x/\nA1\nA2\nyes, f .x/ 2 L2\nno, f .x/ 62 L2\nyes, x 2 L1\nno, x 62 L1\nFigure 34.5\nThe proof of Lemma 34.3. The algorithm F is a reduction algorithm that computes the\nreduction function f from L1 to L2 in polynomial time, and A2 is a polynomial-time algorithm that\ndecides L2. Algorithm A1 decides whether x 2 L1 by using F to transform any input x into f .x/\nand then using A2 to decide whether f .x/ 2 L2.\n\n問題：Explain the concept of a \"polynomial-time reduction\" (≤_P). Why is this concept so central to the theory of NP-completeness?\n\n回答：",
        "llm_final_answer": "根據提供的資料，以下是關於「多項式時間歸約」（polynomial-time reduction，表示為 ≤_P）的概念以及它在 NP-完備性理論中核心地位的解釋：\n\n**多項式時間歸約的概念：**\n\n*   **核心思想：** 多項式時間歸約是一種將一個問題（A）轉換為另一個問題（B）的方法。 它的目的是，如果我們能用多項式時間解決問題 B，我們就能用多項式時間解決問題 A。\n*   **操作步驟：**\n    1.  使用多項式時間歸約演算法將問題 A 的一個實例（instance）轉換為問題 B 的一個實例。\n    2.  在問題 B 的實例上運行多項式時間決策演算法。\n    3.  使用問題 B 的答案作為問題 A 的答案。\n*   **重點：** 這個轉換過程本身必須在多項式時間內完成。\n*   **示例（圖 34.1）：** 想象一下，問題 A 的一個實例被轉換成問題 B 的一個實例。如果問題 B 的答案是「是」，那麼問題 A 的答案也是「是」。反之亦然。\n\n**多項式時間歸約在 NP-完備性理論中的重要性：**\n\n*   **證明問題的難度：** 雖然多項式時間歸約可以用來證明一個問題（A）有多容易（通過證明另一個問題 B 很容易），但它主要用於反向證明一個問題的難度，尤其是在 NP-完備性中。\n*   **證明 NP-完備性：** 證明一個問題是 NP-完備的，就是利用多項式時間歸約。 也就是說，如果一個已知的 NP-完備問題（例如電路可滿足性問題）可以歸約到另一個問題 B，那麼問題 B 至少和已知的 NP-完備問題一樣難，因此問題 B 也是 NP-完備的。\n*   **利用歸約關係：** 多項式時間歸約提供了一種形式化的方式來證明一個問題至少與另一個問題一樣困難，至多相差一個多項式時間的係數。\n\n總而言之，多項式時間歸約是 NP-完備性理論的基石，它允許我們通過將問題之間的難度關聯起來，來分類和研究計算問題的複雜性。",
        "error_message": null,
        "question_id": "EN_NP_REDUCTION_001"
    },
    {
        "original_question": "描述一個「斐波那契堆 (Fibonacci Heap)」的結構。根據本書，它與傳統的「二元堆 (Binary Heap)」相比，其主要優勢體現在哪些操作上？這使其在哪類演算法中特別有用？",
        "decomposed_queries_text": "1. 什麼是斐波那契堆？ (結構定義)\n2. 斐波那契堆的基本結構組成是什麼？\n3. 二元堆的基本結構是什麼？\n4. 斐波那契堆與二元堆相比，在哪些操作上的時間複雜度更優？\n5. 這些時間複雜度優勢是如何實現的？ (簡要說明，不需要深入探討具體實現細節)\n6. 斐波那契堆的優勢使其在哪些演算法中特別有用？",
        "hypothetical_document": "斐波那契堆是一種由多個最小堆組成的資料結構，其採用一組樹狀結構，允許在堆中進行快速的減小鍵值 (decrease-key) 和合併 (merge) 操作。相較於二元堆，斐波那契堆在 decrease-key 操作上具有均攤 O(1) 的時間複雜度，而二元堆則為 O(log n)；該特性使其特別適用於求解稀疏圖上的最短路徑問題，如 Dijkstra 演算法和 Prim 演算法。這種優勢源於其「延遲合併」策略，允許在必要時才進行堆的重組，從而減少了操作的整體時間。",
        "query_for_retrieval": "斐波那契堆是一種由多個最小堆組成的資料結構，其採用一組樹狀結構，允許在堆中進行快速的減小鍵值 (decrease-key) 和合併 (merge) 操作。相較於二元堆，斐波那契堆在 decrease-key 操作上具有均攤 O(1) 的時間複雜度，而二元堆則為 O(log n)；該特性使其特別適用於求解稀疏圖上的最短路徑問題，如 Dijkstra 演算法和 Prim 演算法。這種優勢源於其「延遲合併」策略，允許在必要時才進行堆的重組，從而減少了操作的整體時間。",
        "retrieved_contexts_from_server": [
            {
                "text": "nary binary heaps, as used in heapsort (Chapter 6), work fairly well. Operations\nother than UNION run in worst-case time O.lg n/ on a binary heap. If we need\nto support the UNION operation, however, binary heaps perform poorly. By con-\ncatenating the two arrays that hold the binary heaps to be merged and then running\nBUILD-MIN-HEAP (see Section 6.3), the UNION operation takes ‚.n/ time in the\nworst case.\nFibonacci heaps, on the other hand, have better asymptotic time bounds than\nbinary heaps for the INSERT, UNION, and DECREASE-KEY operations, and they\nhave the same asymptotic running times for the remaining operations. Note, how-\never, that the running times for Fibonacci heaps in Figure 19.1 are amortized time\nbounds, not worst-case per-operation time bounds. The UNION operation takes\nonly constant amortized time in a Fibonacci heap, which is signiﬁcantly better\nthan the linear worst-case time required in a binary heap (assuming, of course, that\nan amortized time bound sufﬁces).\nFibonacci heaps in theory and practice\nFrom a theoretical standpoint, Fibonacci heaps are especially desirable when the\nnumber of EXTRACT-MIN and DELETE operations is small relative to the number\nof other operations performed. This situation arises in many applications. For\nexample, some algorithms for graph problems may call DECREASE-KEY once per\nedge. For dense graphs, which have many edges, the ‚.1/ amortized time of each\ncall of DECREASE-KEY adds up to a big improvement over the ‚.lg n/ worst-case\ntime of binary heaps. Fast algorithms for problems such as computing minimum\nspanning trees (Chapter 23) and ﬁnding single-source shortest paths (Chapter 24)\nmake essential use of Fibonacci heaps.\n19.1\nStructure of Fibonacci heaps\n507\nFrom a practical point of view, however, the constant factors and program-\nming complexity of Fibonacci heaps make them less desirable than ordinary binary\n(or k-ary) heaps for most applications, except for certain applications that manage",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_062219b8-46e4-4ec0-9eb3-035299b4d11f"
                }
            },
            {
                "text": "19.1\nStructure of Fibonacci heaps\n507\nFrom a practical point of view, however, the constant factors and program-\nming complexity of Fibonacci heaps make them less desirable than ordinary binary\n(or k-ary) heaps for most applications, except for certain applications that manage\nlarge amounts of data. Thus, Fibonacci heaps are predominantly of theoretical in-\nterest. If a much simpler data structure with the same amortized time bounds as\nFibonacci heaps were developed, it would be of practical use as well.\nBoth binary heaps and Fibonacci heaps are inefﬁcient in how they support the\noperation SEARCH; it can take a while to ﬁnd an element with a given key. For this\nreason, operations such as DECREASE-KEY and DELETE that refer to a given ele-\nment require a pointer to that element as part of their input. As in our discussion of\npriority queues in Section 6.5, when we use a mergeable heap in an application, we\noften store a handle to the corresponding application object in each mergeable-heap\nelement, as well as a handle to the corresponding mergeable-heap element in each\napplication object. The exact nature of these handles depends on the application\nand its implementation.\nLike several other data structures that we have seen, Fibonacci heaps are based\non rooted trees. We represent each element by a node within a tree, and each\nnode has a key attribute. For the remainder of this chapter, we shall use the term\n“node” instead of “element.” We shall also ignore issues of allocating nodes prior\nto insertion and freeing nodes following deletion, assuming instead that the code\ncalling the heap procedures deals with these details.\nSection 19.1 deﬁnes Fibonacci heaps, discusses how we represent them, and\npresents the potential function used for their amortized analysis.\nSection 19.2\nshows how to implement the mergeable-heap operations and achieve the amortized\ntime bounds shown in Figure 19.1. The remaining two operations, DECREASE-",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_6b447b0e-5917-4770-9e02-e74fd73414ec"
                }
            },
            {
                "text": ".V \u0005 Q; Q/ (with the exception of the ﬁrst iteration, in which u D r due to line 4).\nRemoving u from the set Q adds it to the set V \u0005 Q of vertices in the tree, thus\nadding .u; u:\u0006/ to A. The for loop of lines 8–11 updates the key and \u0006 attributes\nof every vertex \u0003 adjacent to u but not in the tree, thereby maintaining the third\npart of the loop invariant.\nThe running time of Prim’s algorithm depends on how we implement the min-\npriority queue Q. If we implement Q as a binary min-heap (see Chapter 6), we\ncan use the BUILD-MIN-HEAP procedure to perform lines 1–5 in O.V / time. The\nbody of the while loop executes jV j times, and since each EXTRACT-MIN opera-\ntion takes O.lg V / time, the total time for all calls to EXTRACT-MIN is O.V lg V /.\nThe for loop in lines 8–11 executes O.E/ times altogether, since the sum of the\nlengths of all adjacency lists is 2 jEj. Within the for loop, we can implement the\ntest for membership in Q in line 9 in constant time by keeping a bit for each vertex\nthat tells whether or not it is in Q, and updating the bit when the vertex is removed\nfrom Q. The assignment in line 11 involves an implicit DECREASE-KEY opera-\ntion on the min-heap, which a binary min-heap supports in O.lg V / time. Thus,\nthe total time for Prim’s algorithm is O.V lg V C E lg V / D O.E lg V /, which is\nasymptotically the same as for our implementation of Kruskal’s algorithm.\nWe can improve the asymptotic running time of Prim’s algorithm by using Fi-\nbonacci heaps. Chapter 19 shows that if a Fibonacci heap holds jV j elements, an\nEXTRACT-MIN operation takes O.lg V / amortized time and a DECREASE-KEY\noperation (to implement line 11) takes O.1/ amortized time. Therefore, if we use a\nFibonacci heap to implement the min-priority queue Q, the running time of Prim’s\nalgorithm improves to O.E C V lg V /.\n23.2\nThe algorithms of Kruskal and Prim\n637\nExercises\n23.2-1\nKruskal’s algorithm can return different spanning trees for the same input graph G,",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_b7f95b7b-1f9c-4ddf-a5ab-18fd3fe351da"
                }
            },
            {
                "text": "all-pairs shortest paths, weighted bipartite matching, and the minimum-spanning-\ntree problem.\nSubsequently, Driscoll, Gabow, Shrairman, and Tarjan [96] developed “relaxed\nheaps” as an alternative to Fibonacci heaps. They devised two varieties of re-\nlaxed heaps. One gives the same amortized time bounds as Fibonacci heaps. The\nother allows DECREASE-KEY to run in O.1/ worst-case (not amortized) time and\nEXTRACT-MIN and DELETE to run in O.lg n/ worst-case time. Relaxed heaps\nalso have some advantages over Fibonacci heaps in parallel algorithms.\nSee also the chapter notes for Chapter 6 for other data structures that support fast\nDECREASE-KEY operations when the sequence of values returned by EXTRACT-\nMIN calls are monotonically increasing over time and the data are integers in a\nspeciﬁc range.\n20\nvan Emde Boas Trees\nIn previous chapters, we saw data structures that support the operations of a priority\nqueue—binary heaps in Chapter 6, red-black trees in Chapter 13,1 and Fibonacci\nheaps in Chapter 19. In each of these data structures, at least one important op-\neration took O.lg n/ time, either worst case or amortized. In fact, because each\nof these data structures bases its decisions on comparing keys, the \u0004.n lg n/ lower\nbound for sorting in Section 8.1 tells us that at least one operation will have to\ntake \u0004.lg n/ time. Why? If we could perform both the INSERT and EXTRACT-MIN\noperations in o.lg n/ time, then we could sort n keys in o.n lg n/ time by ﬁrst per-\nforming n INSERT operations, followed by n EXTRACT-MIN operations.\nWe saw in Chapter 8, however, that sometimes we can exploit additional infor-\nmation about the keys to sort in o.n lg n/ time. In particular, with counting sort\nwe can sort n keys, each an integer in the range 0 to k, in time ‚.n C k/, which\nis ‚.n/ when k D O.n/.\nSince we can circumvent the \u0004.n lg n/ lower bound for sorting when the keys are\nintegers in a bounded range, you might wonder whether we can perform each of the",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_9379793e-d641-4b99-b06c-1817a3070c4f"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nnary binary heaps, as used in heapsort (Chapter 6), work fairly well. Operations\nother than UNION run in worst-case time O.lg n/ on a binary heap. If we need\nto support the UNION operation, however, binary heaps perform poorly. By con-\ncatenating the two arrays that hold the binary heaps to be merged and then running\nBUILD-MIN-HEAP (see Section 6.3), the UNION operation takes ‚.n/ time in the\nworst case.\nFibonacci heaps, on the other hand, have better asymptotic time bounds than\nbinary heaps for the INSERT, UNION, and DECREASE-KEY operations, and they\nhave the same asymptotic running times for the remaining operations. Note, how-\never, that the running times for Fibonacci heaps in Figure 19.1 are amortized time\nbounds, not worst-case per-operation time bounds. The UNION operation takes\nonly constant amortized time in a Fibonacci heap, which is signiﬁcantly better\nthan the linear worst-case time required in a binary heap (assuming, of course, that\nan amortized time bound sufﬁces).\nFibonacci heaps in theory and practice\nFrom a theoretical standpoint, Fibonacci heaps are especially desirable when the\nnumber of EXTRACT-MIN and DELETE operations is small relative to the number\nof other operations performed. This situation arises in many applications. For\nexample, some algorithms for graph problems may call DECREASE-KEY once per\nedge. For dense graphs, which have many edges, the ‚.1/ amortized time of each\ncall of DECREASE-KEY adds up to a big improvement over the ‚.lg n/ worst-case\ntime of binary heaps. Fast algorithms for problems such as computing minimum\nspanning trees (Chapter 23) and ﬁnding single-source shortest paths (Chapter 24)\nmake essential use of Fibonacci heaps.\n19.1\nStructure of Fibonacci heaps\n507\nFrom a practical point of view, however, the constant factors and program-\nming complexity of Fibonacci heaps make them less desirable than ordinary binary\n(or k-ary) heaps for most applications, except for certain applications that manage\n\n19.1\nStructure of Fibonacci heaps\n507\nFrom a practical point of view, however, the constant factors and program-\nming complexity of Fibonacci heaps make them less desirable than ordinary binary\n(or k-ary) heaps for most applications, except for certain applications that manage\nlarge amounts of data. Thus, Fibonacci heaps are predominantly of theoretical in-\nterest. If a much simpler data structure with the same amortized time bounds as\nFibonacci heaps were developed, it would be of practical use as well.\nBoth binary heaps and Fibonacci heaps are inefﬁcient in how they support the\noperation SEARCH; it can take a while to ﬁnd an element with a given key. For this\nreason, operations such as DECREASE-KEY and DELETE that refer to a given ele-\nment require a pointer to that element as part of their input. As in our discussion of\npriority queues in Section 6.5, when we use a mergeable heap in an application, we\noften store a handle to the corresponding application object in each mergeable-heap\nelement, as well as a handle to the corresponding mergeable-heap element in each\napplication object. The exact nature of these handles depends on the application\nand its implementation.\nLike several other data structures that we have seen, Fibonacci heaps are based\non rooted trees. We represent each element by a node within a tree, and each\nnode has a key attribute. For the remainder of this chapter, we shall use the term\n“node” instead of “element.” We shall also ignore issues of allocating nodes prior\nto insertion and freeing nodes following deletion, assuming instead that the code\ncalling the heap procedures deals with these details.\nSection 19.1 deﬁnes Fibonacci heaps, discusses how we represent them, and\npresents the potential function used for their amortized analysis.\nSection 19.2\nshows how to implement the mergeable-heap operations and achieve the amortized\ntime bounds shown in Figure 19.1. The remaining two operations, DECREASE-\n\n.V \u0005 Q; Q/ (with the exception of the ﬁrst iteration, in which u D r due to line 4).\nRemoving u from the set Q adds it to the set V \u0005 Q of vertices in the tree, thus\nadding .u; u:\u0006/ to A. The for loop of lines 8–11 updates the key and \u0006 attributes\nof every vertex \u0003 adjacent to u but not in the tree, thereby maintaining the third\npart of the loop invariant.\nThe running time of Prim’s algorithm depends on how we implement the min-\npriority queue Q. If we implement Q as a binary min-heap (see Chapter 6), we\ncan use the BUILD-MIN-HEAP procedure to perform lines 1–5 in O.V / time. The\nbody of the while loop executes jV j times, and since each EXTRACT-MIN opera-\ntion takes O.lg V / time, the total time for all calls to EXTRACT-MIN is O.V lg V /.\nThe for loop in lines 8–11 executes O.E/ times altogether, since the sum of the\nlengths of all adjacency lists is 2 jEj. Within the for loop, we can implement the\ntest for membership in Q in line 9 in constant time by keeping a bit for each vertex\nthat tells whether or not it is in Q, and updating the bit when the vertex is removed\nfrom Q. The assignment in line 11 involves an implicit DECREASE-KEY opera-\ntion on the min-heap, which a binary min-heap supports in O.lg V / time. Thus,\nthe total time for Prim’s algorithm is O.V lg V C E lg V / D O.E lg V /, which is\nasymptotically the same as for our implementation of Kruskal’s algorithm.\nWe can improve the asymptotic running time of Prim’s algorithm by using Fi-\nbonacci heaps. Chapter 19 shows that if a Fibonacci heap holds jV j elements, an\nEXTRACT-MIN operation takes O.lg V / amortized time and a DECREASE-KEY\noperation (to implement line 11) takes O.1/ amortized time. Therefore, if we use a\nFibonacci heap to implement the min-priority queue Q, the running time of Prim’s\nalgorithm improves to O.E C V lg V /.\n23.2\nThe algorithms of Kruskal and Prim\n637\nExercises\n23.2-1\nKruskal’s algorithm can return different spanning trees for the same input graph G,\n\nall-pairs shortest paths, weighted bipartite matching, and the minimum-spanning-\ntree problem.\nSubsequently, Driscoll, Gabow, Shrairman, and Tarjan [96] developed “relaxed\nheaps” as an alternative to Fibonacci heaps. They devised two varieties of re-\nlaxed heaps. One gives the same amortized time bounds as Fibonacci heaps. The\nother allows DECREASE-KEY to run in O.1/ worst-case (not amortized) time and\nEXTRACT-MIN and DELETE to run in O.lg n/ worst-case time. Relaxed heaps\nalso have some advantages over Fibonacci heaps in parallel algorithms.\nSee also the chapter notes for Chapter 6 for other data structures that support fast\nDECREASE-KEY operations when the sequence of values returned by EXTRACT-\nMIN calls are monotonically increasing over time and the data are integers in a\nspeciﬁc range.\n20\nvan Emde Boas Trees\nIn previous chapters, we saw data structures that support the operations of a priority\nqueue—binary heaps in Chapter 6, red-black trees in Chapter 13,1 and Fibonacci\nheaps in Chapter 19. In each of these data structures, at least one important op-\neration took O.lg n/ time, either worst case or amortized. In fact, because each\nof these data structures bases its decisions on comparing keys, the \u0004.n lg n/ lower\nbound for sorting in Section 8.1 tells us that at least one operation will have to\ntake \u0004.lg n/ time. Why? If we could perform both the INSERT and EXTRACT-MIN\noperations in o.lg n/ time, then we could sort n keys in o.n lg n/ time by ﬁrst per-\nforming n INSERT operations, followed by n EXTRACT-MIN operations.\nWe saw in Chapter 8, however, that sometimes we can exploit additional infor-\nmation about the keys to sort in o.n lg n/ time. In particular, with counting sort\nwe can sort n keys, each an integer in the range 0 to k, in time ‚.n C k/, which\nis ‚.n/ when k D O.n/.\nSince we can circumvent the \u0004.n lg n/ lower bound for sorting when the keys are\nintegers in a bounded range, you might wonder whether we can perform each of the\n\n問題：描述一個「斐波那契堆 (Fibonacci Heap)」的結構。根據本書，它與傳統的「二元堆 (Binary Heap)」相比，其主要優勢體現在哪些操作上？這使其在哪類演算法中特別有用？\n\n回答：",
        "llm_final_answer": "根據提供的資料，斐波那契堆是一種基於樹狀結構的資料結構，它使用節點來表示元素，每個節點都有一個 key 屬性。\n\n與二元堆相比，斐波那契堆在某些操作上具有更好的漸近時間界限，特別是在 **INSERT、UNION 和 DECREASE-KEY** 操作上。具體來說，UNION 操作在斐波那契堆中只需常數攤銷時間，而二元堆需要線性時間。\n\n斐波那契堆在 EXTRACT-MIN 和 DELETE 操作數量相對較少的應用中尤其有用。例如，在一些圖論演算法中，例如**計算最小生成樹（Chapter 23）和尋找單源最短路徑（Chapter 24）**，斐波那契堆可以提高演算法的效率。",
        "error_message": null,
        "question_id": "ZH_FIBONACCI_HEAP_001"
    },
    {
        "original_question": "The activity-selection problem is a classic example where a greedy algorithm yields an optimal solution. Describe the specific \"greedy choice\" that works for this problem and briefly justify why it leads to a globally optimal solution.",
        "decomposed_queries_text": "1.  **是什麼**：簡要描述活動選擇問題。\n2.  **是什麼**：定義貪婪演算法。\n3.  **是什麼**：描述針對活動選擇問題有效的貪婪選擇策略。\n4.  **為什麼**：解釋為什麼上述貪婪選擇策略能夠產生全域最佳解。",
        "hypothetical_document": "在活動選擇問題中，貪婪選擇策略是按照活動的完成時間進行排序，並優先選擇最早完成時間的活動。 這種貪婪策略之所以有效，是因為它在每一步都選擇了剩餘活動中最具潛力的活動，最大化了可用時間，從而確保了全局最優解，其時間複雜度為 `O(n log n)`，主要消耗在排序環節。",
        "query_for_retrieval": "在活動選擇問題中，貪婪選擇策略是按照活動的完成時間進行排序，並優先選擇最早完成時間的活動。 這種貪婪策略之所以有效，是因為它在每一步都選擇了剩餘活動中最具潛力的活動，最大化了可用時間，從而確保了全局最優解，其時間複雜度為 `O(n log n)`，主要消耗在排序環節。",
        "retrieved_contexts_from_server": [
            {
                "text": "choice in mind, so that the choice leaves just one subproblem to solve. In the\nactivity-selection problem, we could have started by dropping the second subscript\nand deﬁning subproblems of the form Sk. Then, we could have proven that a greedy\nchoice (the ﬁrst activity am to ﬁnish in Sk), combined with an optimal solution to\nthe remaining set Sm of compatible activities, yields an optimal solution to Sk.\nMore generally, we design greedy algorithms according to the following sequence\nof steps:\n1. Cast the optimization problem as one in which we make a choice and are left\nwith one subproblem to solve.\n2. Prove that there is always an optimal solution to the original problem that makes\nthe greedy choice, so that the greedy choice is always safe.\n424\nChapter 16\nGreedy Algorithms\n3. Demonstrate optimal substructure by showing that, having made the greedy\nchoice, what remains is a subproblem with the property that if we combine an\noptimal solution to the subproblem with the greedy choice we have made, we\narrive at an optimal solution to the original problem.\nWe shall use this more direct process in later sections of this chapter. Neverthe-\nless, beneath every greedy algorithm, there is almost always a more cumbersome\ndynamic-programming solution.\nHow can we tell whether a greedy algorithm will solve a particular optimization\nproblem? No way works all the time, but the greedy-choice property and optimal\nsubstructure are the two key ingredients. If we can demonstrate that the problem\nhas these properties, then we are well on the way to developing a greedy algorithm\nfor it.\nGreedy-choice property\nThe ﬁrst key ingredient is the greedy-choice property: we can assemble a globally\noptimal solution by making locally optimal (greedy) choices. In other words, when\nwe are considering which choice to make, we make the choice that looks best in\nthe current problem, without considering results from subproblems.\nHere is where greedy algorithms differ from dynamic programming. In dynamic",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_e4038cd4-a0bf-487e-ad1d-57197b96aaf5"
                }
            },
            {
                "text": "We can usually make the greedy choice more efﬁciently than when we have to\nconsider a wider set of choices. For example, in the activity-selection problem, as-\n16.2\nElements of the greedy strategy\n425\nsuming that we had already sorted the activities in monotonically increasing order\nof ﬁnish times, we needed to examine each activity just once. By preprocessing the\ninput or by using an appropriate data structure (often a priority queue), we often\ncan make greedy choices quickly, thus yielding an efﬁcient algorithm.\nOptimal substructure\nA problem exhibits optimal substructure if an optimal solution to the problem\ncontains within it optimal solutions to subproblems. This property is a key in-\ngredient of assessing the applicability of dynamic programming as well as greedy\nalgorithms. As an example of optimal substructure, recall how we demonstrated in\nSection 16.1 that if an optimal solution to subproblem Sij includes an activity ak,\nthen it must also contain optimal solutions to the subproblems Sik and Skj . Given\nthis optimal substructure, we argued that if we knew which activity to use as ak, we\ncould construct an optimal solution to Sij by selecting ak along with all activities\nin optimal solutions to the subproblems Sik and Skj . Based on this observation of\noptimal substructure, we were able to devise the recurrence (16.2) that described\nthe value of an optimal solution.\nWe usually use a more direct approach regarding optimal substructure when\napplying it to greedy algorithms. As mentioned above, we have the luxury of\nassuming that we arrived at a subproblem by having made the greedy choice in\nthe original problem. All we really need to do is argue that an optimal solution to\nthe subproblem, combined with the greedy choice already made, yields an optimal\nsolution to the original problem. This scheme implicitly uses induction on the\nsubproblems to prove that making the greedy choice at every step produces an\noptimal solution.\nGreedy versus dynamic programming",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_e9923fa3-59ae-443b-a321-06cebc43d20f"
                }
            },
            {
                "text": "activities such that P\nak2A \u0003k is maximized. Give a polynomial-time algorithm for\nthis problem.\n16.2\nElements of the greedy strategy\n423\n16.2\nElements of the greedy strategy\nA greedy algorithm obtains an optimal solution to a problem by making a sequence\nof choices. At each decision point, the algorithm makes choice that seems best at\nthe moment. This heuristic strategy does not always produce an optimal solution,\nbut as we saw in the activity-selection problem, sometimes it does. This section\ndiscusses some of the general properties of greedy methods.\nThe process that we followed in Section 16.1 to develop a greedy algorithm was\na bit more involved than is typical. We went through the following steps:\n1. Determine the optimal substructure of the problem.\n2. Develop a recursive solution. (For the activity-selection problem, we formu-\nlated recurrence (16.2), but we bypassed developing a recursive algorithm based\non this recurrence.)\n3. Show that if we make the greedy choice, then only one subproblem remains.\n4. Prove that it is always safe to make the greedy choice. (Steps 3 and 4 can occur\nin either order.)\n5. Develop a recursive algorithm that implements the greedy strategy.\n6. Convert the recursive algorithm to an iterative algorithm.\nIn going through these steps, we saw in great detail the dynamic-programming un-\nderpinnings of a greedy algorithm. For example, in the activity-selection problem,\nwe ﬁrst deﬁned the subproblems Sij, where both i and j varied. We then found\nthat if we always made the greedy choice, we could restrict the subproblems to be\nof the form Sk.\nAlternatively, we could have fashioned our optimal substructure with a greedy\nchoice in mind, so that the choice leaves just one subproblem to solve. In the\nactivity-selection problem, we could have started by dropping the second subscript\nand deﬁning subproblems of the form Sk. Then, we could have proven that a greedy",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_b494755e-ef7b-494f-a195-b4a06d599dc4"
                }
            },
            {
                "text": "we are considering which choice to make, we make the choice that looks best in\nthe current problem, without considering results from subproblems.\nHere is where greedy algorithms differ from dynamic programming. In dynamic\nprogramming, we make a choice at each step, but the choice usually depends on the\nsolutions to subproblems. Consequently, we typically solve dynamic-programming\nproblems in a bottom-up manner, progressing from smaller subproblems to larger\nsubproblems. (Alternatively, we can solve them top down, but memoizing. Of\ncourse, even though the code works top down, we still must solve the subprob-\nlems before making a choice.) In a greedy algorithm, we make whatever choice\nseems best at the moment and then solve the subproblem that remains. The choice\nmade by a greedy algorithm may depend on choices so far, but it cannot depend on\nany future choices or on the solutions to subproblems. Thus, unlike dynamic pro-\ngramming, which solves the subproblems before making the ﬁrst choice, a greedy\nalgorithm makes its ﬁrst choice before solving any subproblems.\nA dynamic-\nprogramming algorithm proceeds bottom up, whereas a greedy strategy usually\nprogresses in a top-down fashion, making one greedy choice after another, reduc-\ning each given problem instance to a smaller one.\nOf course, we must prove that a greedy choice at each step yields a globally\noptimal solution. Typically, as in the case of Theorem 16.1, the proof examines\na globally optimal solution to some subproblem. It then shows how to modify\nthe solution to substitute the greedy choice for some other choice, resulting in one\nsimilar, but smaller, subproblem.\nWe can usually make the greedy choice more efﬁciently than when we have to\nconsider a wider set of choices. For example, in the activity-selection problem, as-\n16.2\nElements of the greedy strategy\n425\nsuming that we had already sorted the activities in monotonically increasing order",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_a7a4efa4-dcdb-4461-8a67-deb540d40c40"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nchoice in mind, so that the choice leaves just one subproblem to solve. In the\nactivity-selection problem, we could have started by dropping the second subscript\nand deﬁning subproblems of the form Sk. Then, we could have proven that a greedy\nchoice (the ﬁrst activity am to ﬁnish in Sk), combined with an optimal solution to\nthe remaining set Sm of compatible activities, yields an optimal solution to Sk.\nMore generally, we design greedy algorithms according to the following sequence\nof steps:\n1. Cast the optimization problem as one in which we make a choice and are left\nwith one subproblem to solve.\n2. Prove that there is always an optimal solution to the original problem that makes\nthe greedy choice, so that the greedy choice is always safe.\n424\nChapter 16\nGreedy Algorithms\n3. Demonstrate optimal substructure by showing that, having made the greedy\nchoice, what remains is a subproblem with the property that if we combine an\noptimal solution to the subproblem with the greedy choice we have made, we\narrive at an optimal solution to the original problem.\nWe shall use this more direct process in later sections of this chapter. Neverthe-\nless, beneath every greedy algorithm, there is almost always a more cumbersome\ndynamic-programming solution.\nHow can we tell whether a greedy algorithm will solve a particular optimization\nproblem? No way works all the time, but the greedy-choice property and optimal\nsubstructure are the two key ingredients. If we can demonstrate that the problem\nhas these properties, then we are well on the way to developing a greedy algorithm\nfor it.\nGreedy-choice property\nThe ﬁrst key ingredient is the greedy-choice property: we can assemble a globally\noptimal solution by making locally optimal (greedy) choices. In other words, when\nwe are considering which choice to make, we make the choice that looks best in\nthe current problem, without considering results from subproblems.\nHere is where greedy algorithms differ from dynamic programming. In dynamic\n\nWe can usually make the greedy choice more efﬁciently than when we have to\nconsider a wider set of choices. For example, in the activity-selection problem, as-\n16.2\nElements of the greedy strategy\n425\nsuming that we had already sorted the activities in monotonically increasing order\nof ﬁnish times, we needed to examine each activity just once. By preprocessing the\ninput or by using an appropriate data structure (often a priority queue), we often\ncan make greedy choices quickly, thus yielding an efﬁcient algorithm.\nOptimal substructure\nA problem exhibits optimal substructure if an optimal solution to the problem\ncontains within it optimal solutions to subproblems. This property is a key in-\ngredient of assessing the applicability of dynamic programming as well as greedy\nalgorithms. As an example of optimal substructure, recall how we demonstrated in\nSection 16.1 that if an optimal solution to subproblem Sij includes an activity ak,\nthen it must also contain optimal solutions to the subproblems Sik and Skj . Given\nthis optimal substructure, we argued that if we knew which activity to use as ak, we\ncould construct an optimal solution to Sij by selecting ak along with all activities\nin optimal solutions to the subproblems Sik and Skj . Based on this observation of\noptimal substructure, we were able to devise the recurrence (16.2) that described\nthe value of an optimal solution.\nWe usually use a more direct approach regarding optimal substructure when\napplying it to greedy algorithms. As mentioned above, we have the luxury of\nassuming that we arrived at a subproblem by having made the greedy choice in\nthe original problem. All we really need to do is argue that an optimal solution to\nthe subproblem, combined with the greedy choice already made, yields an optimal\nsolution to the original problem. This scheme implicitly uses induction on the\nsubproblems to prove that making the greedy choice at every step produces an\noptimal solution.\nGreedy versus dynamic programming\n\nactivities such that P\nak2A \u0003k is maximized. Give a polynomial-time algorithm for\nthis problem.\n16.2\nElements of the greedy strategy\n423\n16.2\nElements of the greedy strategy\nA greedy algorithm obtains an optimal solution to a problem by making a sequence\nof choices. At each decision point, the algorithm makes choice that seems best at\nthe moment. This heuristic strategy does not always produce an optimal solution,\nbut as we saw in the activity-selection problem, sometimes it does. This section\ndiscusses some of the general properties of greedy methods.\nThe process that we followed in Section 16.1 to develop a greedy algorithm was\na bit more involved than is typical. We went through the following steps:\n1. Determine the optimal substructure of the problem.\n2. Develop a recursive solution. (For the activity-selection problem, we formu-\nlated recurrence (16.2), but we bypassed developing a recursive algorithm based\non this recurrence.)\n3. Show that if we make the greedy choice, then only one subproblem remains.\n4. Prove that it is always safe to make the greedy choice. (Steps 3 and 4 can occur\nin either order.)\n5. Develop a recursive algorithm that implements the greedy strategy.\n6. Convert the recursive algorithm to an iterative algorithm.\nIn going through these steps, we saw in great detail the dynamic-programming un-\nderpinnings of a greedy algorithm. For example, in the activity-selection problem,\nwe ﬁrst deﬁned the subproblems Sij, where both i and j varied. We then found\nthat if we always made the greedy choice, we could restrict the subproblems to be\nof the form Sk.\nAlternatively, we could have fashioned our optimal substructure with a greedy\nchoice in mind, so that the choice leaves just one subproblem to solve. In the\nactivity-selection problem, we could have started by dropping the second subscript\nand deﬁning subproblems of the form Sk. Then, we could have proven that a greedy\n\nwe are considering which choice to make, we make the choice that looks best in\nthe current problem, without considering results from subproblems.\nHere is where greedy algorithms differ from dynamic programming. In dynamic\nprogramming, we make a choice at each step, but the choice usually depends on the\nsolutions to subproblems. Consequently, we typically solve dynamic-programming\nproblems in a bottom-up manner, progressing from smaller subproblems to larger\nsubproblems. (Alternatively, we can solve them top down, but memoizing. Of\ncourse, even though the code works top down, we still must solve the subprob-\nlems before making a choice.) In a greedy algorithm, we make whatever choice\nseems best at the moment and then solve the subproblem that remains. The choice\nmade by a greedy algorithm may depend on choices so far, but it cannot depend on\nany future choices or on the solutions to subproblems. Thus, unlike dynamic pro-\ngramming, which solves the subproblems before making the ﬁrst choice, a greedy\nalgorithm makes its ﬁrst choice before solving any subproblems.\nA dynamic-\nprogramming algorithm proceeds bottom up, whereas a greedy strategy usually\nprogresses in a top-down fashion, making one greedy choice after another, reduc-\ning each given problem instance to a smaller one.\nOf course, we must prove that a greedy choice at each step yields a globally\noptimal solution. Typically, as in the case of Theorem 16.1, the proof examines\na globally optimal solution to some subproblem. It then shows how to modify\nthe solution to substitute the greedy choice for some other choice, resulting in one\nsimilar, but smaller, subproblem.\nWe can usually make the greedy choice more efﬁciently than when we have to\nconsider a wider set of choices. For example, in the activity-selection problem, as-\n16.2\nElements of the greedy strategy\n425\nsuming that we had already sorted the activities in monotonically increasing order\n\n問題：The activity-selection problem is a classic example where a greedy algorithm yields an optimal solution. Describe the specific \"greedy choice\" that works for this problem and briefly justify why it leads to a globally optimal solution.\n\n回答：",
        "llm_final_answer": "根據提供的資料，在活動選擇問題中，貪心選擇是選擇最早完成的活動。 為了證明這個選擇會得到全局最優解，我們需要證明兩個關鍵屬性：\n\n*   **貪心選擇性質：** 選擇最早完成的活動，使我們能夠儘可能多地安排其他活動。\n*   **最優子結構：** 在做出貪心選擇後，剩餘的活動選擇問題具有最優子結構。也就是說，對於子問題的最優解，與貪心選擇組合起來，能產生原始問題的最優解。\n\n透過使用貪心選擇和最優子結構，我們可以構建出一個全局最優解。",
        "error_message": null,
        "question_id": "EN_GREEDY_ACTIVITY_SELECTION_001"
    },
    {
        "original_question": "請說明「Rabin-Karp 演算法」在字串匹配中的基本思想，特別是它如何利用「雜湊 (hashing)」來有效率地在大文本中尋找模式字串。",
        "decomposed_queries_text": "1.  什麼是字串匹配？\n2.  Rabin-Karp 演算法的整體目標是什麼？\n3.  Rabin-Karp 演算法如何利用雜湊（hashing）？\n4.  什麼是 Rolling Hash (滾動雜湊)？ Rolling Hash 在 Rabin-Karp 演算法中扮演什麼角色？\n5.  Rabin-Karp 演算法的基本步驟是什麼？\n6.  Rabin-Karp 演算法如何處理雜湊碰撞？\n7.  Rabin-Karp 演算法的優點是什麼？\n8.  Rabin-Karp 演算法的缺點是什麼？\n9.  與其他字串匹配演算法相比，Rabin-Karp 演算法的效率如何？",
        "hypothetical_document": "Rabin-Karp 演算法利用雜湊函數進行字串匹配，透過計算模式字串與文本子串的雜湊值，以 `O(1)` 時間複雜度進行快速比較。若雜湊值匹配，則進一步比較字串本身以確認實際匹配，避免了 `O(mn)` 的暴力搜尋，但因雜湊衝突，最壞時間複雜度仍為 `O(mn)`，平均複雜度則接近 `O(n)`。其效率受益於滾動雜湊，可在 `O(1)` 時間內更新子串的雜湊值。",
        "query_for_retrieval": "Rabin-Karp 演算法利用雜湊函數進行字串匹配，透過計算模式字串與文本子串的雜湊值，以 `O(1)` 時間複雜度進行快速比較。若雜湊值匹配，則進一步比較字串本身以確認實際匹配，避免了 `O(mn)` 的暴力搜尋，但因雜湊衝突，最壞時間複雜度仍為 `O(mn)`，平均複雜度則接近 `O(n)`。其效率受益於滾動雜湊，可在 `O(1)` 時間內更新子串的雜湊值。",
        "retrieved_contexts_from_server": [
            {
                "text": "ulo q acts like a random mapping from †\u0004 to Zq. (See the discussion on the use of\ndivision for hashing in Section 11.3.1. It is difﬁcult to formalize and prove such an\nassumption, although one viable approach is to assume that q is chosen randomly\nfrom integers of the appropriate size. We shall not pursue this formalization here.)\nWe can then expect that the number of spurious hits is O.n=q/, since we can es-\ntimate the chance that an arbitrary ts will be equivalent to p, modulo q, as 1=q.\nSince there are O.n/ positions at which the test of line 10 fails and we spend O.m/\ntime for each hit, the expected matching time taken by the Rabin-Karp algorithm\nis\nO.n/ C O.m.\u0003 C n=q// ;\nwhere \u0003 is the number of valid shifts. This running time is O.n/ if \u0003 D O.1/ and\nwe choose q \u0006 m. That is, if the expected number of valid shifts is small (O.1/)\nand we choose the prime q to be larger than the length of the pattern, then we\ncan expect the Rabin-Karp procedure to use only O.n C m/ matching time. Since\nm \u0002 n, this expected matching time is O.n/.\nExercises\n32.2-1\nWorking modulo q D 11, how many spurious hits does the Rabin-Karp matcher en-\ncounter in the text T D 3141592653589793 when looking for the pattern P D 26?\n32.2-2\nHow would you extend the Rabin-Karp method to the problem of searching a text\nstring for an occurrence of any one of a given set of k patterns? Start by assuming\nthat all k patterns have the same length. Then generalize your solution to allow the\npatterns to have different lengths.\n32.2-3\nShow how to extend the Rabin-Karp method to handle the problem of looking for\na given m \t m pattern in an n \t n array of characters. (The pattern may be shifted\nvertically and horizontally, but it may not be rotated.)\n32.3\nString matching with ﬁnite automata\n995\n32.2-4\nAlice has a copy of a long n-bit ﬁle A D han\u00031; an\u00032; : : : ; a0i, and Bob similarly\nhas an n-bit ﬁle B D hbn\u00031; bn\u00032; : : : ; b0i. Alice and Bob wish to know if their",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_2aba9d25-e9ad-4d82-b3b2-0b358fdebbe5"
                }
            },
            {
                "text": "respectively, from the d-ary alphabet †d D f0; 1; : : : ; d \u0005 1g, where d \u0006 2. Show\nthat the expected number of character-to-character comparisons made by the im-\nplicit loop in line 4 of the naive algorithm is\n.n \u0005 m C 1/1 \u0005 d \u0003m\n1 \u0005 d \u00031 \u0002 2.n \u0005 m C 1/\nover all executions of this loop. (Assume that the naive algorithm stops comparing\ncharacters for a given shift once it ﬁnds a mismatch or matches the entire pattern.)\nThus, for randomly chosen strings, the naive algorithm is quite efﬁcient.\n32.1-4\nSuppose we allow the pattern P to contain occurrences of a gap character } that\ncan match an arbitrary string of characters (even one of zero length). For example,\nthe pattern ab}ba}c occurs in the text cabccbacbacab as\nc ab’\nab\ncc’\n}\nba’\nba\ncba\n“\n}\nc’\nc\nab\nand as\nc ab’\nab\nccbac\n—\n}\nba’\nba ’\n}\nc’\nc\nab :\n990\nChapter 32\nString Matching\nNote that the gap character may occur an arbitrary number of times in the pattern\nbut not at all in the text. Give a polynomial-time algorithm to determine whether\nsuch a pattern P occurs in a given text T , and analyze the running time of your\nalgorithm.\n32.2\nThe Rabin-Karp algorithm\nRabin and Karp proposed a string-matching algorithm that performs well in prac-\ntice and that also generalizes to other algorithms for related problems, such as\ntwo-dimensional pattern matching. The Rabin-Karp algorithm uses ‚.m/ prepro-\ncessing time, and its worst-case running time is ‚..n\u0005mC1/m/. Based on certain\nassumptions, however, its average-case running time is better.\nThis algorithm makes use of elementary number-theoretic notions such as the\nequivalence of two numbers modulo a third number. You might want to refer to\nSection 31.1 for the relevant deﬁnitions.\nFor expository purposes, let us assume that † D f0; 1; 2; : : : ; 9g, so that each\ncharacter is a decimal digit. (In the general case, we can assume that each charac-\nter is a digit in radix-d notation, where d D j†j.) We can then view a string of k",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_83706ebc-899f-4a76-b834-e3f8fced95ef"
                }
            },
            {
                "text": "interpreted as radix-d digits. The subscripts on t are provided only for clarity; the\nprogram works correctly if all the subscripts are dropped. Line 3 initializes h to the\nvalue of the high-order digit position of an m-digit window. Lines 4–8 compute p\nas the value of P Œ1 : : m\u0002 mod q and t0 as the value of T Œ1 : : m\u0002 mod q. The for\nloop of lines 9–14 iterates through all possible shifts s, maintaining the following\ninvariant:\nWhenever line 10 is executed, ts D T Œs C 1 : : s C m\u0002 mod q.\nIf p D ts in line 10 (a “hit”), then line 11 checks to see whether P Œ1 : : m\u0002 D\nT Œs C1 : : s Cm\u0002 in order to rule out the possibility of a spurious hit. Line 12 prints\nout any valid shifts that are found. If s < n \u0005 m (checked in line 13), then the for\nloop will execute at least one more time, and so line 14 ﬁrst executes to ensure that\nthe loop invariant holds when we get back to line 10. Line 14 computes the value\nof tsC1 mod q from the value of ts mod q in constant time using equation (32.2)\ndirectly.\nRABIN-KARP-MATCHER takes ‚.m/ preprocessing time, and its matching time\nis ‚..n \u0005 m C 1/m/ in the worst case, since (like the naive string-matching algo-\nrithm) the Rabin-Karp algorithm explicitly veriﬁes every valid shift. If P D am\n994\nChapter 32\nString Matching\nand T D an, then verifying takes time ‚..n\u0005mC1/m/, since each of the n\u0005mC1\npossible shifts is valid.\nIn many applications, we expect few valid shifts—perhaps some constant c of\nthem. In such applications, the expected matching time of the algorithm is only\nO..n \u0005 m C 1/ C cm/ D O.n C m/, plus the time required to process spurious\nhits. We can base a heuristic analysis on the assumption that reducing values mod-\nulo q acts like a random mapping from †\u0004 to Zq. (See the discussion on the use of\ndivision for hashing in Section 11.3.1. It is difﬁcult to formalize and prove such an\nassumption, although one viable approach is to assume that q is chosen randomly",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_f1d7dba3-6e87-4913-8106-795047ff51d4"
                }
            },
            {
                "text": "shaded number, computed modulo 13, yields the value 7. (b) The same text string with values com-\nputed modulo 13 for each possible position of a length-5 window. Assuming the pattern P D 31415,\nwe look for windows whose value modulo 13 is 7, since 31415 \b 7 .mod 13/. The algorithm ﬁnds\ntwo such windows, shown shaded in the ﬁgure. The ﬁrst, beginning at text position 7, is indeed an\noccurrence of the pattern, while the second, beginning at text position 13, is a spurious hit. (c) How\nto compute the value for a window in constant time, given the value for the previous window. The\nﬁrst window has value 31415. Dropping the high-order digit 3, shifting left (multiplying by 10), and\nthen adding in the low-order digit 2 gives us the new value 14152. Because all computations are\nperformed modulo 13, the value for the ﬁrst window is 7, and the value for the new window is 8.\n32.2\nThe Rabin-Karp algorithm\n993\nP Œ1 : : m\u0002 D T Œs C 1 : : s C m\u0002. If q is large enough, then we hope that spurious\nhits occur infrequently enough that the cost of the extra checking is low.\nThe following procedure makes these ideas precise. The inputs to the procedure\nare the text T , the pattern P , the radix d to use (which is typically taken to be j†j),\nand the prime q to use.\nRABIN-KARP-MATCHER.T; P; d; q/\n1\nn D T:length\n2\nm D P:length\n3\nh D d m\u00031 mod q\n4\np D 0\n5\nt0 D 0\n6\nfor i D 1 to m\n// preprocessing\n7\np D .dp C P Œi\u0002/ mod q\n8\nt0 D .dt0 C T Œi\u0002/ mod q\n9\nfor s D 0 to n \u0005 m\n// matching\n10\nif p == ts\n11\nif P Œ1 : : m\u0002 == T Œs C 1 : : s C m\u0002\n12\nprint “Pattern occurs with shift” s\n13\nif s < n \u0005 m\n14\ntsC1 D .d.ts \u0005 T Œs C 1\u0002h/ C T Œs C m C 1\u0002/ mod q\nThe procedure RABIN-KARP-MATCHER works as follows. All characters are\ninterpreted as radix-d digits. The subscripts on t are provided only for clarity; the\nprogram works correctly if all the subscripts are dropped. Line 3 initializes h to the\nvalue of the high-order digit position of an m-digit window. Lines 4–8 compute p",
                "metadata": {
                    "source": "./Introduction_to_algorithms-3rd Edition.pdf",
                    "retrieved_parent_id": "parent_4dd945b0-402f-4796-8a27-232ca4f9bc80"
                }
            }
        ],
        "final_prompt_to_llm": "作為一位知識淵博且樂於助人的AI老師，請你嚴格根據以下提供的「上下文」資訊來回答「問題」。你的回答應該清晰、易懂，並且有條理，就像在向學生解釋概念一樣。如果可能，可以適當使用點列、步驟說明或簡短的例子來幫助理解，但所有解釋都必須基於「上下文」。如果「上下文」中沒有明確提及與「問題」直接相關的內容，請回答「根據提供的資料，我無法找到關於此問題的直接資訊」。請絕對不要使用任何「上下文」之外的知識。\n\n上下文：\nulo q acts like a random mapping from †\u0004 to Zq. (See the discussion on the use of\ndivision for hashing in Section 11.3.1. It is difﬁcult to formalize and prove such an\nassumption, although one viable approach is to assume that q is chosen randomly\nfrom integers of the appropriate size. We shall not pursue this formalization here.)\nWe can then expect that the number of spurious hits is O.n=q/, since we can es-\ntimate the chance that an arbitrary ts will be equivalent to p, modulo q, as 1=q.\nSince there are O.n/ positions at which the test of line 10 fails and we spend O.m/\ntime for each hit, the expected matching time taken by the Rabin-Karp algorithm\nis\nO.n/ C O.m.\u0003 C n=q// ;\nwhere \u0003 is the number of valid shifts. This running time is O.n/ if \u0003 D O.1/ and\nwe choose q \u0006 m. That is, if the expected number of valid shifts is small (O.1/)\nand we choose the prime q to be larger than the length of the pattern, then we\ncan expect the Rabin-Karp procedure to use only O.n C m/ matching time. Since\nm \u0002 n, this expected matching time is O.n/.\nExercises\n32.2-1\nWorking modulo q D 11, how many spurious hits does the Rabin-Karp matcher en-\ncounter in the text T D 3141592653589793 when looking for the pattern P D 26?\n32.2-2\nHow would you extend the Rabin-Karp method to the problem of searching a text\nstring for an occurrence of any one of a given set of k patterns? Start by assuming\nthat all k patterns have the same length. Then generalize your solution to allow the\npatterns to have different lengths.\n32.2-3\nShow how to extend the Rabin-Karp method to handle the problem of looking for\na given m \t m pattern in an n \t n array of characters. (The pattern may be shifted\nvertically and horizontally, but it may not be rotated.)\n32.3\nString matching with ﬁnite automata\n995\n32.2-4\nAlice has a copy of a long n-bit ﬁle A D han\u00031; an\u00032; : : : ; a0i, and Bob similarly\nhas an n-bit ﬁle B D hbn\u00031; bn\u00032; : : : ; b0i. Alice and Bob wish to know if their\n\nrespectively, from the d-ary alphabet †d D f0; 1; : : : ; d \u0005 1g, where d \u0006 2. Show\nthat the expected number of character-to-character comparisons made by the im-\nplicit loop in line 4 of the naive algorithm is\n.n \u0005 m C 1/1 \u0005 d \u0003m\n1 \u0005 d \u00031 \u0002 2.n \u0005 m C 1/\nover all executions of this loop. (Assume that the naive algorithm stops comparing\ncharacters for a given shift once it ﬁnds a mismatch or matches the entire pattern.)\nThus, for randomly chosen strings, the naive algorithm is quite efﬁcient.\n32.1-4\nSuppose we allow the pattern P to contain occurrences of a gap character } that\ncan match an arbitrary string of characters (even one of zero length). For example,\nthe pattern ab}ba}c occurs in the text cabccbacbacab as\nc ab’\nab\ncc’\n}\nba’\nba\ncba\n“\n}\nc’\nc\nab\nand as\nc ab’\nab\nccbac\n—\n}\nba’\nba ’\n}\nc’\nc\nab :\n990\nChapter 32\nString Matching\nNote that the gap character may occur an arbitrary number of times in the pattern\nbut not at all in the text. Give a polynomial-time algorithm to determine whether\nsuch a pattern P occurs in a given text T , and analyze the running time of your\nalgorithm.\n32.2\nThe Rabin-Karp algorithm\nRabin and Karp proposed a string-matching algorithm that performs well in prac-\ntice and that also generalizes to other algorithms for related problems, such as\ntwo-dimensional pattern matching. The Rabin-Karp algorithm uses ‚.m/ prepro-\ncessing time, and its worst-case running time is ‚..n\u0005mC1/m/. Based on certain\nassumptions, however, its average-case running time is better.\nThis algorithm makes use of elementary number-theoretic notions such as the\nequivalence of two numbers modulo a third number. You might want to refer to\nSection 31.1 for the relevant deﬁnitions.\nFor expository purposes, let us assume that † D f0; 1; 2; : : : ; 9g, so that each\ncharacter is a decimal digit. (In the general case, we can assume that each charac-\nter is a digit in radix-d notation, where d D j†j.) We can then view a string of k\n\ninterpreted as radix-d digits. The subscripts on t are provided only for clarity; the\nprogram works correctly if all the subscripts are dropped. Line 3 initializes h to the\nvalue of the high-order digit position of an m-digit window. Lines 4–8 compute p\nas the value of P Œ1 : : m\u0002 mod q and t0 as the value of T Œ1 : : m\u0002 mod q. The for\nloop of lines 9–14 iterates through all possible shifts s, maintaining the following\ninvariant:\nWhenever line 10 is executed, ts D T Œs C 1 : : s C m\u0002 mod q.\nIf p D ts in line 10 (a “hit”), then line 11 checks to see whether P Œ1 : : m\u0002 D\nT Œs C1 : : s Cm\u0002 in order to rule out the possibility of a spurious hit. Line 12 prints\nout any valid shifts that are found. If s < n \u0005 m (checked in line 13), then the for\nloop will execute at least one more time, and so line 14 ﬁrst executes to ensure that\nthe loop invariant holds when we get back to line 10. Line 14 computes the value\nof tsC1 mod q from the value of ts mod q in constant time using equation (32.2)\ndirectly.\nRABIN-KARP-MATCHER takes ‚.m/ preprocessing time, and its matching time\nis ‚..n \u0005 m C 1/m/ in the worst case, since (like the naive string-matching algo-\nrithm) the Rabin-Karp algorithm explicitly veriﬁes every valid shift. If P D am\n994\nChapter 32\nString Matching\nand T D an, then verifying takes time ‚..n\u0005mC1/m/, since each of the n\u0005mC1\npossible shifts is valid.\nIn many applications, we expect few valid shifts—perhaps some constant c of\nthem. In such applications, the expected matching time of the algorithm is only\nO..n \u0005 m C 1/ C cm/ D O.n C m/, plus the time required to process spurious\nhits. We can base a heuristic analysis on the assumption that reducing values mod-\nulo q acts like a random mapping from †\u0004 to Zq. (See the discussion on the use of\ndivision for hashing in Section 11.3.1. It is difﬁcult to formalize and prove such an\nassumption, although one viable approach is to assume that q is chosen randomly\n\nshaded number, computed modulo 13, yields the value 7. (b) The same text string with values com-\nputed modulo 13 for each possible position of a length-5 window. Assuming the pattern P D 31415,\nwe look for windows whose value modulo 13 is 7, since 31415 \b 7 .mod 13/. The algorithm ﬁnds\ntwo such windows, shown shaded in the ﬁgure. The ﬁrst, beginning at text position 7, is indeed an\noccurrence of the pattern, while the second, beginning at text position 13, is a spurious hit. (c) How\nto compute the value for a window in constant time, given the value for the previous window. The\nﬁrst window has value 31415. Dropping the high-order digit 3, shifting left (multiplying by 10), and\nthen adding in the low-order digit 2 gives us the new value 14152. Because all computations are\nperformed modulo 13, the value for the ﬁrst window is 7, and the value for the new window is 8.\n32.2\nThe Rabin-Karp algorithm\n993\nP Œ1 : : m\u0002 D T Œs C 1 : : s C m\u0002. If q is large enough, then we hope that spurious\nhits occur infrequently enough that the cost of the extra checking is low.\nThe following procedure makes these ideas precise. The inputs to the procedure\nare the text T , the pattern P , the radix d to use (which is typically taken to be j†j),\nand the prime q to use.\nRABIN-KARP-MATCHER.T; P; d; q/\n1\nn D T:length\n2\nm D P:length\n3\nh D d m\u00031 mod q\n4\np D 0\n5\nt0 D 0\n6\nfor i D 1 to m\n// preprocessing\n7\np D .dp C P Œi\u0002/ mod q\n8\nt0 D .dt0 C T Œi\u0002/ mod q\n9\nfor s D 0 to n \u0005 m\n// matching\n10\nif p == ts\n11\nif P Œ1 : : m\u0002 == T Œs C 1 : : s C m\u0002\n12\nprint “Pattern occurs with shift” s\n13\nif s < n \u0005 m\n14\ntsC1 D .d.ts \u0005 T Œs C 1\u0002h/ C T Œs C m C 1\u0002/ mod q\nThe procedure RABIN-KARP-MATCHER works as follows. All characters are\ninterpreted as radix-d digits. The subscripts on t are provided only for clarity; the\nprogram works correctly if all the subscripts are dropped. Line 3 initializes h to the\nvalue of the high-order digit position of an m-digit window. Lines 4–8 compute p\n\n問題：請說明「Rabin-Karp 演算法」在字串匹配中的基本思想，特別是它如何利用「雜湊 (hashing)」來有效率地在大文本中尋找模式字串。\n\n回答：",
        "llm_final_answer": "Rabin-Karp 演算法是一種字串匹配演算法，它在實務上表現良好，並且可以泛化到其他相關問題的演算法，例如二維模式匹配。\n\n以下是 Rabin-Karp 演算法在字串匹配中的基本思想：\n\n1.  **將字串轉換為數字：** 演算法將字串中的每個字元都視為一個數字（例如，使用基數-d 符號，其中 d 是字母表的基數）。它將長度為 m 的字串轉換為一個數字，這個數字可以通過計算字串中字元的加權和（模 q）得到。\n\n2.  **計算模式字串的雜湊值：** 演算法首先計算模式字串 P 的雜湊值。\n\n3.  **計算文本字串中每個可能位置的雜湊值：** 演算法接著計算文本字串 T 中每個可能位置的長度為 m 的子字串的雜湊值。\n\n4.  **比較雜湊值：** 演算法將模式字串的雜湊值與文本字串中每個子字串的雜湊值進行比較。\n\n5.  **避免假陽性（Spurious Hits）：** 如果兩個雜湊值相等，這不一定意味著模式字串與子字串完全匹配，因為不同的字串可能具有相同的雜湊值（稱為「碰撞」）。因此，當雜湊值匹配時，演算法會進行額外的字串比較以確認實際匹配。\n\n6.  **高效計算雜湊值：** 演算法使用一個技巧，可以在常數時間內從前一個子字串的雜湊值計算出下一個子字串的雜湊值，這利用了公式 (32.2)。\n\n透過使用雜湊，Rabin-Karp 演算法可以在很多情況下快速地排除不匹配的子字串，從而提高字串匹配的效率。",
        "error_message": null,
        "question_id": "ZH_RABIN_KARP_001"
    }
]